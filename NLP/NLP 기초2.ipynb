{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 기초2\n",
    "\n",
    "NLP에 대한 기본적인 지식들을 담은 쥬비터 노트북입니다.  \n",
    "-참고 사이트 https://wikidocs.net/book/2155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 언어 모델(Language Model)\n",
    "\n",
    "## 1. 언어 모델(Language Model)이란?\n",
    "\n",
    "### 1. 언어 모델(Language Model)이란?\n",
    "\n",
    "언어 모델을 간단하게 요약하면 1. 문장의 확률을 계산하거나, 또는 2. 이전 단어들이 주어졌을 때 다음 단어가 나올 확률을 계산하는 것을 말합니다.\n",
    "\n",
    "### 2. 문장의 확률을 계산하는 일이 왜 필요한가?\n",
    "\n",
    "1. 기계 번역(Machine Translation)  \n",
    "    P(나는 버스를 탔다) > P(나는 버스를 태운다)  \n",
    "    여기서 P는 언어 모델\n",
    "    \n",
    "\n",
    "2. 오타 교정(Spell Correction)  \n",
    "    선생님이 교실로 부리나케  \n",
    "    P(달려갔다) > P(잘려갔다)\n",
    "\n",
    "\n",
    "3. 음성 인식(Speech Recognition)  \n",
    "    P(나는 메롱을 먹는다) < P(나는 메론을 먹는다)\n",
    "    \n",
    "### 3. 언어 모델은 문장의 확률 또는 단어 등장 확률을 계산하는 일\n",
    "\n",
    "1. 문장 또는 단어의 나열의 확률\n",
    "$$P(W) = P(w_1, w_2, w_3, \\cdots, w_n)\n",
    "$$\n",
    "\n",
    "여기서 $w_i$는 하나의 단어이고, 여러 단어가 모여 생긴 전체 문장을 대문자 $W$입니다.\n",
    "\n",
    "2. 단어들의 나열이 주어졌을때 다음에 나타날 단어의 확률\n",
    "\n",
    "$$ P(w_5 | w_1, w_2, w_3, w_4) \n",
    "$$\n",
    "\n",
    "단어들의 나열이 주어졌을때 다음에 나타날 단어의 확률은 이런 식으로 조건부 확률로 구합니다.\n",
    "\n",
    "### 4. 언어 모델의 간단한 직관\n",
    "\n",
    "가령, __비행기를 타려고 공항에 갔는데 지각을 하는 바람에 비행기를 [?]__ 라는 문장이 있다고 합시다. 우리는 손쉽게 '놓쳤다'라고 예상할 수 있습니다. 그렇다면 컴퓨터에서 위의 문장을 주고나서, '비행기를' 다음에 나올 단어를 예측해보라고 한다면 컴퓨터는 앞에 어떤 단어들이 나왔는지 고려하여, 후보가 될 수 있는 여러 단어들에 대해서 확률을 계산해보고 가장 높은 확률을 가진 단어를 선택합니다. 이 때 사용되는 것이 바로 언어 모델 입니다.\n",
    "\n",
    "## 2. 확률 언어 모델(Statistical Language Model)\n",
    "\n",
    "\n",
    "먼저 예를 들어보겠습니다.An adorable little boy is spreading smiles이라는 문장입니다. 우리는 이 문장 하나에 대한 전체 확률을 구해보겠습니다. 즉, $P(An, adorable, little, boy, is, spreading, smiles)$를 계산하고자 합니다.\n",
    "    \n",
    "각 단어는 문맥이라는 관계로 인해 이전 단어의 영향을 받아 나온 단어입니다. 그렇기 때문에 우리는 이 문장 전체의 확률을 구하고자 조건부 확률을 사용합니다.\n",
    "$$P(w_1, w_2, \\cdots, w_n) = \\Pi_i P(w_i|w_1w_2\\cdots w_{i-1})$$\n",
    "    \n",
    "실제 예제에 해당 식을 적용하면 다음과 같습니다.\n",
    "$$P(\\text{An adorable little boy is spreading smiles}) =\\\\ \n",
    "    P(An) \\times P(adorable|An) \\times \\cdots \\times P(\\text{smiles}|\\text{An adorable little boy is spreading})\n",
    "$$\n",
    "    \n",
    "### 1. 확률은 카운트(count) 기반으로 접근한다.\n",
    "\n",
    "이 경우 확률은 카운트에 기반하여 확률을 계산합니다. 즉, 다시말해 An adorable little boy가 나왔을 때, is가 나올 확률인 $P(\\text{is}|\\text{An adorable little boy})$를 계산하는 식은 다음과 같습니다. \n",
    "\n",
    "$$P(\\text{is}|\\text{An adorable little boy}) = \\\\\n",
    "\\frac{Count(\\text{An adorable little boy is})}{Count(\\text{An adorable little boy})}\n",
    "$$\n",
    "\n",
    "### 2. 카운트 기반의 접근의 한계\n",
    "\n",
    "언어 모델에 대해서 다시 재정의 해보자면, 실생활에서 사용되는 말의 정확한 확률 분포를 근사 모델링하는 것이 언어 모델이라고 볼 수 있습니다.  \n",
    "우리는 여러 코퍼스를 통해 훈련하여, LM을 통해 현실에서의 확률 모델을 근사하는 것이 우리의 목표입니다. 그런데 카운트 기반으로 접근하려고 한다면, 우리가 갖고있는 코퍼스에는 엄청난 데이터가 있어야 합니다. 만약 우리가 An adorable little boy is라는 문장 자체가 없었다고 하면 우리가 갖고있는 확률을 계산할 수 없습니다. 하지만 현실에선 An adorable little boy is라는 단어의 나열이 분명이 존재할 수 있습니다.  \n",
    "우리는 이런 문제를 해결하기위해 n-gram을 이용한 언어 모델과 여러가지 일반화 기법을 사용할 수 있습니다.\n",
    "\n",
    "## 3. N-gram Laguage Model\n",
    "\n",
    "An adorable littel boy가 나왔을 때 is가 나올 확률을 구하기 위해서는 An adorable little boy가 나온 횟수와, An adorable little boy is가 나온 횟수를 카운트해야만 했지만 이제는 단어의 확률을 구하고자, 기준 단어의 앞 단어를 전부 포함해서 세지말고, 앞 단어 중 임의의 개수만 포함해서 셀 수 있습니다. 그러면 갖고있는 코퍼스에서도 해당 단어의 나열을 카운트할 확률이 높아집니다.\n",
    "\n",
    "### 1. N-gram\n",
    "\n",
    "n-gram은 n개의 연속적인 단어 나열을 의미합니다. 갖고 있는 코퍼스에서 n개의 단어 뭉치 단위로 끊어서 자연어 처리를 진행하는 것입니다. n-gram을 사용할 때는 n이 1일 때는 유니그램(unigram), 2일 때는 바이그램(bigram), 3일 때는 트라이그램(trigram)이라고 명명하고, n이 4 이상일 때는 gram 앞에 그대로 숫자를 붙여서 명명합니다. 주어진 문제에서 4-gram을 이용한 언어 모델을 사용한다고 생각해봅시다. 이 경우, spreading 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려합니다.\n",
    "\n",
    "$$P(w|\\text{boy is spreading}) = \\frac{Count(\\text{boy is spreading w})}{Count(\\text{boy is spreading})} \n",
    "$$\n",
    "\n",
    "### 2. N-gram Language Model의 한계\n",
    "\n",
    "#### 1. n을 선택하는 것은 trade-off 문제입니다.\n",
    "\n",
    "n을 높이면 높일수록 정확도는 올라갈 수 있지만, 훈련 코퍼스에서 해당 데이터를 카운트할 수 있는 확률은 적어집니다. n을 작게 잡으면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 실제의 확률분포와 멀어집니다. 일반적으로 n은 최대 5를 넘게 잡아서는 안 된다고 권장됩니다.\n",
    "\n",
    "#### 2. 카운트 했을 때 0이 되는 문제(zero count problem)\\\n",
    "\n",
    "n-gram 언어 모델도 카운트를 할 수 있는 확률을 조금 높여줄 뿐이지 여전히 카운트가 되지 않을 가능성이 존재합니다.\n",
    "\n",
    "## 4. 펄플렉서티(Perplexity)\n",
    "\n",
    "### 1. 언어 모델의 평가 방법: PPL(perplexity)\n",
    "\n",
    "펄플렉서티(perplexity)는 언어 모델을 평가하기 위한 내부 평가 지표입니다. 보통 줄여서 PPL 또는 PP라고 표현합니다. PPL은 낮을 수록 언어 모델의 성능이 좋다는 것을 의미합니다. PPL의 의미는 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하는 지입니다.\n",
    "\n",
    "### 2. 펄플렉서티(Perplexity)의 수식\n",
    "\n",
    "펄플렉서티(Perplexity)의 수식은 아래와 같습니다.\n",
    "\n",
    "$$PPL = 2^{H(P)} = 2^{-\\sum_x p(x)log_2 P(x)}\n",
    "$$\n",
    "\n",
    "여기서 $x$는 선택가능한 경우의 수 중 하나이며, $p(x)$는 $x$의 확률을 나타냅니다. 만약 모든 $x$의 확률이 $1/x$로 같다면, PPL은 1이 됩니다. 만약 모든 $x$ 중에서 하나의 $x$의 확률이 1이고, 나머지 $x$의 확률이 0이라면 PPL은 1이 됩니다. 이 경우에는 수식상으로는 언어 모델이 다음 단어를 말할 때 100% 확신을 가진다는 뜻입니다.\n",
    "\n",
    "\n",
    "참고로 $H(P)$는 크로스 엔트로피 수식입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
