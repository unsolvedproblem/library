{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 토픽 모델링(Topic Modeling)\n",
    "\n",
    "토픽 모델링(Topic Modeling)이란 기계 학습 및 자연어 처리 분야에서 토픽 모델(Topic Model)이라는 문서 집합의 추상적인 주제를 발견하기 위한 통계적 모델 중 하나로, 텍스트 본문의 숨겨진 의미구조를 발견하기 위해 사용되는 텍스트 마이닝 기법입니다.\n",
    "\n",
    "### 1. 잠재 의미 분석(Latent Semantic Analysis, LSA)\n",
    "\n",
    "BoW에 기반한 단어 문서 행렬이나 TF-IDF는 기본적으로 단어의 빈도 수를 이용한 수치화 방법이기 때문에 단어의 의미를 고려하지 못합니다(이를 토픽 모델링 관점에서는 단어의 토픽을 고려하지 못한다고 합니다). 이를 위한 대안으로 단어 문서 행렬의 잠재된(Latent) 의미를 이끌어내는 방법으로 잠재 의미 분석(Latent Semantic Analysis)이라는 방법이 있습니다. 이 방법은 선형대수학의 특이값 분해(Singular Value Decomposition)를 통해 행할 수 있습니다.\n",
    "\n",
    "#### 1. Reduced SVD\n",
    "\n",
    "SVD에서 일부 벡터들을 삭제하는 것을 데이터 차원을 줄인다고도 말하는데, 데이터의 차원을 줄이게 되면 계산 비용이 낮아지는 효과를 얻을 수 있습니다. 또, 상대적으로 중요하지 않은 정보를 삭제하는 효과를 갖고 있는데, 이는 영상 처리 분야에서는 노이즈를 제거한다는 의미를 갖고, 자연어 처리 분야에서는 설명력이 낮은 정보를 삭제한다는 의미를 갖고 있습니다.\n",
    "\n",
    "#### 2. 잠재 의미 분석(Latent Semantic Analysis, LSA)\n",
    "\n",
    "기존의 단어 문서 행렬이나 단어 문서 행렬에 단어의 중요도에 따른 가중치를 주었던 TF-IDF 행렬은 단어의 의미를 전혀 고려하지 못했습니다. LSA는 기본적으로 단어 문서 행렬이나 TF-IDF 행렬에 SVD를 사용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 끌어낸다는 아이디어를 갖고 있습니다.\n",
    "\n",
    "#### 3. 실습을 통한 이해\n",
    "\n",
    "사이킷 런에서는 Twenty Newsgroups라는 20개의 다른 주제를 가진 뉴스 데이터를 제공합니다. 이제 해당 데이터를 통해서 직접 LSA를 통해 토픽 모델링을 수행해보도록 하겠습니다.\n",
    "\n",
    "##### 1. 뉴스데이터에 대한 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers','quotes'))\n",
    "\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 데이터는 총 11,314개의 데이터를 갖고 있습니다. 이 중 한 데이터를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Yeah, do you expect people to read the FAQ, etc. and actually accept hard\n",
      "atheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\n",
      "of steam!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jim,\n",
      "\n",
      "Sorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\n",
      "denial about the faith you need to get by.  Oh well, just pretend that it will\n",
      "all end happily ever after anyway.  Maybe if you start a new newsgroup,\n",
      "alt.atheist.hard, you won't be bummin' so much?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \n",
      "--\n",
      "Bake Timmons, III\n"
     ]
    }
   ],
   "source": [
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보시다 시피 많은 특수문자가 있는 영어문장으로 구성되어져 있습니다. 사이킷 런이 제공하는 뉴스 데이터에서 target_name에 본래 이 뉴스 데이터가 어떤 20개의 카테고리를 갖고 있었는지가 저장되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 텍스트 전처리\n",
    "\n",
    "1. 알파벳을 제외한 구두점, 숫자, 특수 문자를 공백으로 제거하기\n",
    "2. 길이가 짧은 단어는 제거하기\n",
    "3. 알파벳을 소문자로 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well i'm not sure about the story nad it did s...</td>\n",
       "      <td>well sure about story seem biased what disagre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...</td>\n",
       "      <td>yeah expect people read actually accept hard a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  Well i'm not sure about the story nad it did s...   \n",
       "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
       "\n",
       "                                           clean_doc  \n",
       "0  well sure about story seem biased what disagre...  \n",
       "1  yeah expect people read actually accept hard a...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame(documents, columns=['document'])\n",
    "\n",
    "news_df['clean_doc'] = news_df['document'].str.replace('[^a-zA-Z#]', ' ')\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "\n",
    "news_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 토큰화와 불영어 제거를 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [well, sure, story, seem, biased, disagree, st...\n",
       "1    [yeah, expect, people, read, actually, accept,...\n",
       "2    [although, realize, principle, strongest, poin...\n",
       "3    [notwithstanding, legitimate, fuss, proposal, ...\n",
       "4    [well, change, scoring, playoff, pool, unfortu...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. TF-IDF 행렬 만들기\n",
    "\n",
    "불용어 제거를 위해 토큰화 작업을 수행했지만, TfidfVectorizer는 기본적으로 토큰화가 되어있지 않은 텍스트 데이터를 입력으로 사용합니다. 그렇기 때문에 다시 토큰화를 역으로 하는 작업을 수행해보도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenized_doc = []\n",
    "\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    well sure story seem biased disagree statement...\n",
       "1    yeah expect people read actually accept hard a...\n",
       "2    although realize principle strongest points wo...\n",
       "3    notwithstanding legitimate fuss proposal much ...\n",
       "4    well change scoring playoff pool unfortunately...\n",
       "Name: clean_doc, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 TF-IDF 행렬을 만들텐데 계산 시간 문제로 단어는 1000개만 가지고 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000,\n",
    "                               max_df=0.5, smooth_idf=True)\n",
    "## max_df 단어장에 포함되기 위한 최대 빈도\n",
    "## 피처를 만들 때 0으로 나오는 항목에 대해 작은 값을 \n",
    "## 더해서(스무딩을 해서) 피처를 만들지 아니면 그냥 생성할지를 결정\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smooth_idf = True\n",
    "$$idf = log(\\frac{N+1}{N_w + 1} + 1)$$\n",
    "\n",
    "smooth_idf = False\n",
    "$$idf = log(\\frac{N}{N_w}+1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 토픽 모델링(Topic Modeling)\n",
    "\n",
    "이제 TF-IDF 행렬을 다수의 행렬로 분해해보도록 하겠습니다. 사이킷런의 TruncatedSVD를 사용합니다. 원래 기존 뉴스 데이터 자체가 20개의 다른 뉴스 카테고리를 갖고 있었기 때문에, 텍스트 데이터에 20개의 토픽 모델링을 시도해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd_model  = TruncatedSVD(n_components=20, \n",
    "                          algorithm='randomized', n_iter=100,\n",
    "                         random_state=122)\n",
    "\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.transform(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ability',\n",
       "  'able',\n",
       "  'accept',\n",
       "  'access',\n",
       "  'according',\n",
       "  'account',\n",
       "  'action',\n",
       "  'actions',\n",
       "  'actual',\n",
       "  'actually'],\n",
       " 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[:10], len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 분류된 20개의 토픽 각각에서 가장 중요한 단어 5개씩 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [(1,3), ()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: like know people think good  \n",
      "Topic 2: thanks windows card drive mail  \n",
      "Topic 3: game team year games season  \n",
      "Topic 4: drive scsi hard disk card  \n",
      "Topic 5: windows file window files program  \n",
      "Topic 6: chip government mail space information  \n",
      "Topic 7: like bike know chip sounds  \n",
      "Topic 8: card sale video monitor offer  \n",
      "Topic 9: know card chip video government  \n",
      "Topic 10: good know time bike jesus  \n",
      "Topic 11: think chip good thanks clipper  \n",
      "Topic 12: thanks right problem good bike  \n",
      "Topic 13: good people windows know file  \n",
      "Topic 14: space think know nasa problem  \n",
      "Topic 15: space good card people time  \n",
      "Topic 16: people problem window time game  \n",
      "Topic 17: time bike right windows file  \n",
      "Topic 18: time problem file think israel  \n",
      "Topic 19: file need card files problem  \n",
      "Topic 20: problem file thanks used space  \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "## 단어들\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key=lambda x:x[1], reverse=True)[:5]\n",
    "    print(\"Topic \"+str(i+1)+\": \", end='')\n",
    "    for t in sorted_terms:\n",
    "        print(t[0], end=' ')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. LSA의 장단점\n",
    "\n",
    "정리해보면 LSA는 쉽고 빠르게 구현이 가능할 뿐만 아니라 __단어의 잠재적인 의미__ 를 끌어낼 수 있어 토픽 모델링, 문서의 유사도 계산 등에서 좋은 성능을 보여준다는 장점을 갖고 있습니다. 하지만 SVD의 특성상 이미 계산된 LSA에 새로운 데이터를 추가하여 계산하려고 하면 보통 처음부터 다시 계산해야 합니다. 이는 최근 LSA 대신 word2vec 등 단어의 의미를 벡터화할 수 있는 다른 방법론인 뉴럴 네트워크 기반의 방법론이 각광받는 이유입니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
