{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 기초3\n",
    "\n",
    "NLP에 대한 기본적인 지식들을 담은 쥬비터 노트북입니다.  \n",
    "-참고 사이트 https://wikidocs.net/book/2155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 카운트 기반의 단어 표현(Count based word Representation)\n",
    "\n",
    "우리는 머신 러닝 등의 알고리즘이 적용된 자연어 처리를 위해서는 문자를 숫자로 수치화할 필요가 있습니다.\n",
    "\n",
    "## 1. 다양한 단어의 표현 방법\n",
    "\n",
    "### 1. 단어의 표현 방법\n",
    "\n",
    "단어의 표현 방법은 크게 국소 표현(Local representation) 방법과 분산 표현(Distributed representation) 방법으로 나뉩니다. 국소 표현 방법은 해당 단어만 보고, 특정값을 맵핑하여 단어를 표현하는 방법이고, 분산 표현 방법은 그 단어를 표현하고자 주변 단어의 의미를 참고하여 단어를 표현하는 방법입니다.\n",
    "\n",
    "비슷한 의미로 국소 표현 방법을 이산 표현(Discrete Representation)이라고도 하며, 분산 표현을 연속 표현(Continuous representation)이라고도 합니다.\n",
    "\n",
    "### 2. 단어 표현의 카테고리화\n",
    "\n",
    "![](./images/word_representation.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BoW(Bag of Word)\n",
    "\n",
    "이번 단원에서는 TDM 행렬의 기본이 되는 개념이자, 단어 등장 순서를 무시하는 빈도수 기반의 방법론인 Bag of Words에 대해 학습하겠습니다.\n",
    "\n",
    "### 1. Bag of Words란?\n",
    "\n",
    "Bag of Words란 단어들의 순서는 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 방법입니다. BoW를 만드는 과정은 두 가지 과정으로 생각할 수 있습니다.\n",
    "\n",
    "1. 우선, 각 단어에 고유한 인덱스(index)를 부여합니다.\n",
    "2. 각 인덱스의 위치에 토큰의 등장 횟수를 기록한 벡터(vector)를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'All': 0, 'my': 1, 'cats': 2, 'in': 3, 'a': 4, 'row': 5, 'When': 6, 'cat': 7, 'sits': 8, 'down': 9, 'she': 10, 'looks': 11, 'like': 12, 'Furby': 13, 'toy': 14}\n",
      "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "text1 = 'All my cats in a row'\n",
    "text2 = 'When my cat sits down, she looks like a Furby toy!'\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "token1 = re.sub(\"[^A-Za-z]\", ' ', text1)\n",
    "token2 = re.sub(\"[^A-Za-z]\", ' ', text2)\n",
    "\n",
    "token1 = word_tokenize(token1)\n",
    "token2 = word_tokenize(token2)\n",
    "\n",
    "token = token1 + token2\n",
    "\n",
    "word2index = {}\n",
    "bow = []\n",
    "\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "        \n",
    "        bow.insert(len(word2index), 1)\n",
    "        \n",
    "    else:\n",
    "        index = word2index.get(voca) ## get은 voca를 key로 value 값을 return합니다\n",
    "        bow[index] += 1\n",
    "        \n",
    "print(word2index)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째 출력 결과를 단어 집합(Vocabulary)이라고 부릅니다. 여기서 단어 집합은 단어에 인덱스를 부여하는 일을 합니다. 단어 집합에 따른 BoW는 두번째 출력 결과입니다. 두번쨰 출력 결과를 보면, my의 index는 1이며, my는 2번 언급되었기 때문에 index 1에 해당하는 값이 2임을 알 수 있습니다.\n",
    "\n",
    "### 2. Bag of Words의 다른 예제들\n",
    "\n",
    "만약 text1과 text2를 합친다면 text3라는 'All my cats in a row When my cat sits donw, she looks like a Furby toy!' 문서가 나올 수 있습니다.  text3로 인덱스 할당과 BoW를 만든다면 위에 같은 결과가 나옵니다.  \n",
    "추가로 'text3의 단어 집합에 대한 text1 BoW' 혹은 'text3의 단어 집합에 대한 text2 BoW'를 생각할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text3의 단어 집합에 대한 text1 BoW: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "text3의 단어 집합에 대한 text2 BoW: [0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "text3 = text1 + ' '+ text2\n",
    "\n",
    "token3 = re.sub(\"[^A-Za-z]\", ' ', text3)\n",
    "token3 = word_tokenize(token3)\n",
    "\n",
    "box1 = [0] * len(word2index)\n",
    "box2 = [0] * len(word2index)\n",
    "\n",
    "for voca in word2index.keys():\n",
    "    if voca in token1:\n",
    "        index = word2index.get(voca)\n",
    "        box1[index] += 1\n",
    "    if voca in token2:\n",
    "        index = word2index.get(voca)\n",
    "        box2[index] += 1    \n",
    "        \n",
    "print('text3의 단어 집합에 대한 text1 BoW:', box1)\n",
    "print('text3의 단어 집합에 대한 text2 BoW:', box2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CountVectorizer 클래스로 BoW 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love, because I love you.']\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자세히 보면 알파벳 I는 BoW를 만드는 과정에서 사라졌습니다. 이는 CountVectorizer가 기본적으로 길이가 2이상인 문자에 대해서만 토큰으로 인식하기 때문입니다. 주의할 것은 CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행하고 BoW를 만든다는 점입니다. 따라서 한글에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 불용어를 제거한 BoW 만들기\n",
    "\n",
    "영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원하고 있습니다.\n",
    "\n",
    "#### 1. 사용자가 직접 정의한 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text =[\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=['the', 'a', 'an', 'is', 'not'])\n",
    "\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. CounterVectorizer에서 제공하는 자체 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. nltk에서 지원하는 불용어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words=sw)\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 단어 문서 행렬(Term-Document Matrix)\n",
    "\n",
    "이번 단원에서는 각 문서에 대한 BoW 표현 방법을 통해 서로 다른 문서들의 BoW들을 결합한 표현 방법인 TDM 표현 방법에 대해 배워보도록 하겠습니다. TDM을 통해 서로 다른 문서들을 비교할 수 있게 됩니다.\n",
    "\n",
    "### 1. 단어 문서 행렬(Term-Document Matrix)의 표기법\n",
    "\n",
    "단어 문서 행렬(Term-Document Matrix)이란 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것을 말합니다. 쉽게 말하면 각 문서에 대한 BoW를 하나의 행렬로 만든 것입니다. BoW와 다른 표현 방법이 아니라 BoW 표현 방법 중 하나라고 볼 수 있습니다. 줄여서 TDM이라고 부릅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 단어 집합에 대한 text1 BoW: [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "전체 단어 집합에 대한 text2 BoW: [0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'All': 0, 'my': 1, 'cats': 2, 'in': 3, 'a': 4, 'row': 5, 'When': 6, 'cat': 7, 'sits': 8, 'down': 9, 'she': 10, 'looks': 11, 'like': 12, 'Furby': 13, 'toy': 14}\n"
     ]
    }
   ],
   "source": [
    "text3 = text1 + ' '+ text2\n",
    "\n",
    "token3 = re.sub(\"[^A-Za-z]\", ' ', text3)\n",
    "token3 = word_tokenize(token3)\n",
    "\n",
    "box1 = [0] * len(word2index)\n",
    "box2 = [0] * len(word2index)\n",
    "\n",
    "for voca in word2index.keys():\n",
    "    if voca in token1:\n",
    "        index = word2index.get(voca)\n",
    "        box1[index] += 1\n",
    "    if voca in token2:\n",
    "        index = word2index.get(voca)\n",
    "        box2[index] += 1    \n",
    "        \n",
    "print('전체 단어 집합에 대한 text1 BoW:', box1)\n",
    "print('전체 단어 집합에 대한 text2 BoW:', box2)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "이번 챕터에서는 TDM 내에 있는 각 단어에 대한 중요도를 계산할 수 있는 TF-IDF 가중치에 대해서 알아보겠습니다. TF-IDF를 사용하면, 기존의 TDM을 사용하는 것보다 정확하게 문서들을 비교할 수 있습니다.\n",
    "\n",
    "### 1. TF-IDF(단어 빈도-역 문서 빈도, Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF는 Term Frequency-Inverse Document Frequency의 줄임말로, 단어의 빈도와 역 문서 빈도(나중에 자세히 설명)를 사용하여 TDM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다.  \n",
    "\n",
    "TF-IDF는 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰일 수 있습니다.  \n",
    "\n",
    "TF-IDF는 TF와 IDF를 곱한 값을 의미합니다. 공식으로 들어가기 전에 앞으로 나오는 문자들의 의미입니다.\n",
    "- d: 문서\n",
    "- t: 단어\n",
    "- n: 문서의 총 개수\n",
    "\n",
    "#### 1. tf(d,f): 특정 문서 d에서의 특정 단어 t의 등장 횟수\n",
    "\n",
    "#### 2. df(t): 특정 단어 t가 등장한 문서의 수\n",
    "\n",
    "#### 3. idf(t): df(t)에 반비례하는 수\n",
    "\n",
    "$$idf(d,t) = log\\frac{n}{1+df(t)}$$\n",
    "\n",
    "$log$를 사용하는 이유는, IDF를 DF의 역수로 사용한다면 총 문서의 수가 커질 수록, IDF의 값은 빠른 속도로 증가합니다. 분모에 1을 더해주는 이유는 특정 단어가 전체 문서에서 등장하지 않을 경우에 분모가 0이 되는 상황을 방지하기 위함입니다.\n",
    "\n",
    "### 2. 사이킷런을 이용한 TDM과 TF-IDF 실습\n",
    "\n",
    "이제 실습을 통해 TDM과 TF-IDF를 직접 만들어보도록 하겠습니다. TDM 또한 BoW 행렬이기 때문에, 앞서 BoW 챕터에서 배운 CountVectorizer를 사용하면 간단히 TDM을 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',\n",
    "]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TMD가 완성되었습니다.사이킷런은 TF-IDF를 자동 계싼해주는 TfidVectorizer 클래스를 제공합니다. 사이킷런의 TF-IDF는 우리가 위에서 배웠던 보편적인 TF-IDF식에서 조금 변형된 식을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do',\n",
    "]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW, TDM, TF-IDF 가중치에 대해서 전부 학습했습니다. 그러면 문서들 간의 유사도를 구하기 위한 재료 손질하는 방법을 배운 것입니다. 이제 문서들간의 유사도를 구하는 방법론에 대해서 다음 챕터에서 배워보겠습니다.\n",
    "\n",
    "## 4. 문서 유사도(Document Similarity)\n",
    "\n",
    "사람들이 말하는 문서의 유사도란 문서들 간에 동일한 단어 또는 비슷한 단어가 얼마나 많이 쓰였는지에 달려있습니다.\n",
    "\n",
    "### 1. 코사인 유사도(Cosine Similarity)\n",
    "\n",
    "#### 1. 코사인 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B) / (norm(A) * norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "1.0000000000000002\n",
      "0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "doc1 = np.array([0,1,1,1])\n",
    "doc2 = np.array([1,0,1,1])\n",
    "doc3 = np.array([2,0,2,2])\n",
    "\n",
    "print(cos_sim(doc1, doc2))\n",
    "print(cos_sim(doc2, doc3))\n",
    "print(cos_sim(doc3, doc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 유사도를 이용한 추천 시스템 구현하기\n",
    "\n",
    "캐글에서 사용되었던 [무비 데이터셋](https://www.kaggle.com/rounakbanik/the-movies-dataset)을 가지고 영화 추천 시스템을 만들어보겠습니다. TF-IDF와 코사인 유사도만으로 영화의 줄거리에 기반해서 영화를 추천하는 추천 시스템을 만들 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adult</th>\n",
       "      <th>belongs_to_collection</th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>homepage</th>\n",
       "      <th>id</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>...</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>status</th>\n",
       "      <th>tagline</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 10194, 'name': 'Toy Story Collection', ...</td>\n",
       "      <td>30000000</td>\n",
       "      <td>[{'id': 16, 'name': 'Animation'}, {'id': 35, '...</td>\n",
       "      <td>http://toystory.disney.com/toy-story</td>\n",
       "      <td>862</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>en</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-10-30</td>\n",
       "      <td>373554033.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}]</td>\n",
       "      <td>Released</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Toy Story</td>\n",
       "      <td>False</td>\n",
       "      <td>7.7</td>\n",
       "      <td>5415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65000000</td>\n",
       "      <td>[{'id': 12, 'name': 'Adventure'}, {'id': 14, '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8844</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>en</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>When siblings Judy and Peter discover an encha...</td>\n",
       "      <td>...</td>\n",
       "      <td>1995-12-15</td>\n",
       "      <td>262797249.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>[{'iso_639_1': 'en', 'name': 'English'}, {'iso...</td>\n",
       "      <td>Released</td>\n",
       "      <td>Roll the dice and unleash the excitement!</td>\n",
       "      <td>Jumanji</td>\n",
       "      <td>False</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2413.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   adult                              belongs_to_collection    budget  \\\n",
       "0  False  {'id': 10194, 'name': 'Toy Story Collection', ...  30000000   \n",
       "1  False                                                NaN  65000000   \n",
       "\n",
       "                                              genres  \\\n",
       "0  [{'id': 16, 'name': 'Animation'}, {'id': 35, '...   \n",
       "1  [{'id': 12, 'name': 'Adventure'}, {'id': 14, '...   \n",
       "\n",
       "                               homepage    id    imdb_id original_language  \\\n",
       "0  http://toystory.disney.com/toy-story   862  tt0114709                en   \n",
       "1                                   NaN  8844  tt0113497                en   \n",
       "\n",
       "  original_title                                           overview  \\\n",
       "0      Toy Story  Led by Woody, Andy's toys live happily in his ...   \n",
       "1        Jumanji  When siblings Judy and Peter discover an encha...   \n",
       "\n",
       "     ...     release_date      revenue runtime  \\\n",
       "0    ...       1995-10-30  373554033.0    81.0   \n",
       "1    ...       1995-12-15  262797249.0   104.0   \n",
       "\n",
       "                                    spoken_languages    status  \\\n",
       "0           [{'iso_639_1': 'en', 'name': 'English'}]  Released   \n",
       "1  [{'iso_639_1': 'en', 'name': 'English'}, {'iso...  Released   \n",
       "\n",
       "                                     tagline      title  video vote_average  \\\n",
       "0                                        NaN  Toy Story  False          7.7   \n",
       "1  Roll the dice and unleash the excitement!    Jumanji  False          6.9   \n",
       "\n",
       "  vote_count  \n",
       "0     5415.0  \n",
       "1     2413.0  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('datasets\\movies\\movies_metadata.csv', low_memory=False)\n",
    "data = data.head(20000)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 24 columns):\n",
      "adult                    20000 non-null object\n",
      "belongs_to_collection    2399 non-null object\n",
      "budget                   20000 non-null object\n",
      "genres                   20000 non-null object\n",
      "homepage                 3055 non-null object\n",
      "id                       20000 non-null object\n",
      "imdb_id                  19993 non-null object\n",
      "original_language        19999 non-null object\n",
      "original_title           20000 non-null object\n",
      "overview                 19865 non-null object\n",
      "popularity               19998 non-null object\n",
      "poster_path              19907 non-null object\n",
      "production_companies     19999 non-null object\n",
      "production_countries     19999 non-null object\n",
      "release_date             19983 non-null object\n",
      "revenue                  19998 non-null float64\n",
      "runtime                  19971 non-null float64\n",
      "spoken_languages         19998 non-null object\n",
      "status                   19979 non-null object\n",
      "tagline                  11706 non-null object\n",
      "title                    19998 non-null object\n",
      "video                    19998 non-null object\n",
      "vote_average             19998 non-null float64\n",
      "vote_count               19998 non-null float64\n",
      "dtypes: float64(4), object(20)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저희가 살펴볼 overview 데이터에 결측치가 있습니다. 결측치는 ' '로 채우겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 47487)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['overview']=data['overview'].fillna(' ')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(data['overview'])\n",
    "\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20000개의 영화를 표현하기위해 총 47487개의 단어가 사용되었습니다. 이제 코사인 유사도를 사용하면 바로 문서의 유사도를 구할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코사인 유사도를 구했습니다. 영화의 타이틀과 인덱스를 가진 테이블을 만들어보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title\n",
       "Toy Story                      0\n",
       "Jumanji                        1\n",
       "Grumpier Old Men               2\n",
       "Waiting to Exhale              3\n",
       "Father of the Bride Part II    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = pd.Series(data.index, index=data[\"title\"])\n",
    "indices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inx = indices['Father of the Bride Part II']\n",
    "inx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 선택한 영화에 대해, 코사인 유사도가 overview와 비슷한 10개의 영화를 찾아 내는 함수를 만들게습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    idx = indices[title]\n",
    "    \n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    \n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    sim_scores = sim_scores[1:11]\n",
    "    \n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    \n",
    "    return data['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12481                            The Dark Knight\n",
       "150                               Batman Forever\n",
       "1328                              Batman Returns\n",
       "15511                 Batman: Under the Red Hood\n",
       "585                                       Batman\n",
       "9230          Batman Beyond: Return of the Joker\n",
       "18035                           Batman: Year One\n",
       "19792    Batman: The Dark Knight Returns, Part 1\n",
       "3095                Batman: Mask of the Phantasm\n",
       "10122                              Batman Begins\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(\"The Dark Knight Rises\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리하자면\n",
    "1. 영화 줄거리 corpus를 받음\n",
    "2. 결측치를 전처리함\n",
    "3. TF-IDF를 구함(stop_word='english')\n",
    "4. TF-IDF를 이용해 코사인 유사도를 구함\n",
    "5. 코사인 유사도를 기준으로 비슷한 영화들을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CountVectorizer',\n",
       " 'In',\n",
       " 'Out',\n",
       " 'TfidfVectorizer',\n",
       " '_',\n",
       " '_17',\n",
       " '_18',\n",
       " '_19',\n",
       " '_20',\n",
       " '_21',\n",
       " '_23',\n",
       " '_25',\n",
       " '_27',\n",
       " '_28',\n",
       " '_29',\n",
       " '_33',\n",
       " '_34',\n",
       " '_35',\n",
       " '_36',\n",
       " '_37',\n",
       " '_40',\n",
       " '_43',\n",
       " '_46',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i10',\n",
       " '_i11',\n",
       " '_i12',\n",
       " '_i13',\n",
       " '_i14',\n",
       " '_i15',\n",
       " '_i16',\n",
       " '_i17',\n",
       " '_i18',\n",
       " '_i19',\n",
       " '_i2',\n",
       " '_i20',\n",
       " '_i21',\n",
       " '_i22',\n",
       " '_i23',\n",
       " '_i24',\n",
       " '_i25',\n",
       " '_i26',\n",
       " '_i27',\n",
       " '_i28',\n",
       " '_i29',\n",
       " '_i3',\n",
       " '_i30',\n",
       " '_i31',\n",
       " '_i32',\n",
       " '_i33',\n",
       " '_i34',\n",
       " '_i35',\n",
       " '_i36',\n",
       " '_i37',\n",
       " '_i38',\n",
       " '_i39',\n",
       " '_i4',\n",
       " '_i40',\n",
       " '_i41',\n",
       " '_i42',\n",
       " '_i43',\n",
       " '_i44',\n",
       " '_i45',\n",
       " '_i46',\n",
       " '_i47',\n",
       " '_i5',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_i8',\n",
       " '_i9',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'a',\n",
       " 'corpus',\n",
       " 'cos_sim',\n",
       " 'cosine_sim',\n",
       " 'data',\n",
       " 'doc1',\n",
       " 'doc2',\n",
       " 'doc3',\n",
       " 'dot',\n",
       " 'exit',\n",
       " 'get_ipython',\n",
       " 'get_recommendations',\n",
       " 'indices',\n",
       " 'inx',\n",
       " 'linear_kernel',\n",
       " 'norm',\n",
       " 'np',\n",
       " 'pd',\n",
       " 'quit',\n",
       " 'tfidf',\n",
       " 'tfidf_matrix',\n",
       " 'tfidfv',\n",
       " 'vector']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, cosine_sim, tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 여러가지 유사도 기법\n",
    "\n",
    "문서의 유사도를 구하기 위한 방법으로는 코사인 유사도 외에도 여러가지 방법들이 있습니다.\n",
    "\n",
    "#### 1. 유클리드 거리(Euclidean Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n",
      "3.1622776601683795\n",
      "2.449489742783178\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def dist(x,y):   \n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "doc1 = np.array((2,3,0,1))\n",
    "doc2 = np.array((1,2,3,1))\n",
    "doc3 = np.array((2,1,2,2))\n",
    "docQ = np.array((1,1,0,1))\n",
    "\n",
    "print(dist(doc1,docQ))\n",
    "print(dist(doc2,docQ))\n",
    "print(dist(doc3,docQ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 자카드 유사도(Jaccard Similarity)\n",
    "\n",
    "$$J(doc_1, doc_2) = \\frac{doc_1 \\cap doc_2}{doc_1 \\cup doc_2}$$\n",
    "\n",
    "#### 3. 편집 거리\n",
    "\n",
    "편집 거리는 한 문자열을 다른 문자열로 치환할 때, 필요한 연산의 수를 거리로 표현한 것입니다. 이 때 연산의 수가 많을 수록 거리가 멀다고 생각하고, 문자열에 대해서 삽입, 삭제, 대체, 전치등을 연산이라 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
