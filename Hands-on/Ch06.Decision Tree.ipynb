{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Machine learning study week11-1.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"Ofo21rUB7Cti","colab_type":"text"},"cell_type":"markdown","source":["# Machine Learning Study"]},{"metadata":{"id":"H8XZ86VC7Ctk","colab_type":"text"},"cell_type":"markdown","source":["기본설정"]},{"metadata":{"scrolled":true,"id":"WVFLzuQ_7Ctm","colab_type":"code","colab":{}},"cell_type":"code","source":["# 공통\n","import numpy as np\n","import os\n","\n","# 일관된 출력을 위해 유사난수 초기화\n","np.random.seed(42)\n","\n","# 맷플롯립 설정\n","%matplotlib inline\n","import matplotlib\n","import matplotlib.pyplot as plt\n","plt.rcParams['axes.labelsize'] = 14\n","plt.rcParams['xtick.labelsize'] = 12\n","plt.rcParams['ytick.labelsize'] = 12\n","\n","# 한글출력\n","matplotlib.rc('font', family='NanumBarunGothic')\n","matplotlib.rcParams['axes.unicode_minus'] = False\n","\n","# 작업할 디렉토리\n","PROJECT_ROOT_DIR = \"C:\\\\Python\\\\MLPATH\" ##파이썬 디렉토리 저장\n","CHAPTER_ID = \"decision_trees\"\n","IMAGE_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n","\n","def image_path(fig_id, image_path_k2h=IMAGE_PATH):\n","    return os.path.join(image_path_k2h, fig_id) ##홈 디렉토리에서 "],"execution_count":0,"outputs":[]},{"metadata":{"id":"N1dWERMr7Ctt","colab_type":"text"},"cell_type":"markdown","source":["# Hands - on Machine Learning with Scikit-Learn & TensorFlow "]},{"metadata":{"id":"96IMjvFg7Ctv","colab_type":"text"},"cell_type":"markdown","source":["# Chapter 6. Decision Tree"]},{"metadata":{"id":"F5D69AQy7Ctx","colab_type":"text"},"cell_type":"markdown","source":["6.0. Introduction\n","\n","\n","6.1. Training and Visualizing a Decision Tree\n","\n","\n","6.2. Making Predictions\n","\n","\n","6.3. Estimating Class\n","\n","\n","6.4. The CART Training Algorithm\n","\n","\n","6.5. Computational Complexity\n","\n","\n","6.6. Gini Impurity or Entropy\n","\n","\n","6.7. Regularization Hyperparameters\n","\n","\n","6.8. Regression\n","\n","\n","6.9. Instability"]},{"metadata":{"id":"7LmZmKk87Ctz","colab_type":"text"},"cell_type":"markdown","source":["## 6.0. Introduction"]},{"metadata":{"id":"TpojXSaa7Ct1","colab_type":"text"},"cell_type":"markdown","source":["- Decision Tree: 분류와 회귀 작업 그리고 다중출력 작업도 가능한 머신러닝 알고리즘.\n","- 복잡한 데이터셋도 학습할 수 있음.\n","- 랜덤 포레스트의 기본 구성 요소\n","\n","\n","- 6장에서는 훈련, 시각화, 예측 방법, CART 훈련 알고리즘, 규제, 회귀 문제에 적용, 제약사항을 살펴 본다."]},{"metadata":{"id":"kWBn2juf7Ct3","colab_type":"text"},"cell_type":"markdown","source":["***"]},{"metadata":{"id":"mF4cCL9s7Ct5","colab_type":"text"},"cell_type":"markdown","source":["## 6.1. Training and Visualizing a Decision Tree"]},{"metadata":{"id":"8iUZndzR7Ct6","colab_type":"text"},"cell_type":"markdown","source":["- 4장에서 사용한 붓꽃 데이터셋으로 DecisionTreeClassifier를 훈련 시킴"]},{"metadata":{"id":"l2ULvjuj7Ct8","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","\n","iris = load_iris() ##모듈 안 load_iris는 function으로 정의되어있고, 함수를 불러야 데이터 Bunch를 얻을 수 있음.\n","\n","X = iris.data[:, 2:] ## data attiribute: [sepal length, sepal width, petal legth, petal width] ## 꽃받침, 꽆잎\n","y = iris.target"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bmh3Ln0I7CuD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2968},"outputId":"100060ab-7791-4ddf-93fb-9ff40b8eea5f","executionInfo":{"status":"ok","timestamp":1543032835164,"user_tz":0,"elapsed":1459,"user":{"displayName":"Ewan","photoUrl":"","userId":"17132486724952991358"}}},"cell_type":"code","source":["print(type(iris)) \n","print(iris)  ##Bunch란?"],"execution_count":3,"outputs":[{"output_type":"stream","text":["<class 'sklearn.utils.Bunch'>\n","{'data': array([[5.1, 3.5, 1.4, 0.2],\n","       [4.9, 3. , 1.4, 0.2],\n","       [4.7, 3.2, 1.3, 0.2],\n","       [4.6, 3.1, 1.5, 0.2],\n","       [5. , 3.6, 1.4, 0.2],\n","       [5.4, 3.9, 1.7, 0.4],\n","       [4.6, 3.4, 1.4, 0.3],\n","       [5. , 3.4, 1.5, 0.2],\n","       [4.4, 2.9, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.1],\n","       [5.4, 3.7, 1.5, 0.2],\n","       [4.8, 3.4, 1.6, 0.2],\n","       [4.8, 3. , 1.4, 0.1],\n","       [4.3, 3. , 1.1, 0.1],\n","       [5.8, 4. , 1.2, 0.2],\n","       [5.7, 4.4, 1.5, 0.4],\n","       [5.4, 3.9, 1.3, 0.4],\n","       [5.1, 3.5, 1.4, 0.3],\n","       [5.7, 3.8, 1.7, 0.3],\n","       [5.1, 3.8, 1.5, 0.3],\n","       [5.4, 3.4, 1.7, 0.2],\n","       [5.1, 3.7, 1.5, 0.4],\n","       [4.6, 3.6, 1. , 0.2],\n","       [5.1, 3.3, 1.7, 0.5],\n","       [4.8, 3.4, 1.9, 0.2],\n","       [5. , 3. , 1.6, 0.2],\n","       [5. , 3.4, 1.6, 0.4],\n","       [5.2, 3.5, 1.5, 0.2],\n","       [5.2, 3.4, 1.4, 0.2],\n","       [4.7, 3.2, 1.6, 0.2],\n","       [4.8, 3.1, 1.6, 0.2],\n","       [5.4, 3.4, 1.5, 0.4],\n","       [5.2, 4.1, 1.5, 0.1],\n","       [5.5, 4.2, 1.4, 0.2],\n","       [4.9, 3.1, 1.5, 0.1],\n","       [5. , 3.2, 1.2, 0.2],\n","       [5.5, 3.5, 1.3, 0.2],\n","       [4.9, 3.1, 1.5, 0.1],\n","       [4.4, 3. , 1.3, 0.2],\n","       [5.1, 3.4, 1.5, 0.2],\n","       [5. , 3.5, 1.3, 0.3],\n","       [4.5, 2.3, 1.3, 0.3],\n","       [4.4, 3.2, 1.3, 0.2],\n","       [5. , 3.5, 1.6, 0.6],\n","       [5.1, 3.8, 1.9, 0.4],\n","       [4.8, 3. , 1.4, 0.3],\n","       [5.1, 3.8, 1.6, 0.2],\n","       [4.6, 3.2, 1.4, 0.2],\n","       [5.3, 3.7, 1.5, 0.2],\n","       [5. , 3.3, 1.4, 0.2],\n","       [7. , 3.2, 4.7, 1.4],\n","       [6.4, 3.2, 4.5, 1.5],\n","       [6.9, 3.1, 4.9, 1.5],\n","       [5.5, 2.3, 4. , 1.3],\n","       [6.5, 2.8, 4.6, 1.5],\n","       [5.7, 2.8, 4.5, 1.3],\n","       [6.3, 3.3, 4.7, 1.6],\n","       [4.9, 2.4, 3.3, 1. ],\n","       [6.6, 2.9, 4.6, 1.3],\n","       [5.2, 2.7, 3.9, 1.4],\n","       [5. , 2. , 3.5, 1. ],\n","       [5.9, 3. , 4.2, 1.5],\n","       [6. , 2.2, 4. , 1. ],\n","       [6.1, 2.9, 4.7, 1.4],\n","       [5.6, 2.9, 3.6, 1.3],\n","       [6.7, 3.1, 4.4, 1.4],\n","       [5.6, 3. , 4.5, 1.5],\n","       [5.8, 2.7, 4.1, 1. ],\n","       [6.2, 2.2, 4.5, 1.5],\n","       [5.6, 2.5, 3.9, 1.1],\n","       [5.9, 3.2, 4.8, 1.8],\n","       [6.1, 2.8, 4. , 1.3],\n","       [6.3, 2.5, 4.9, 1.5],\n","       [6.1, 2.8, 4.7, 1.2],\n","       [6.4, 2.9, 4.3, 1.3],\n","       [6.6, 3. , 4.4, 1.4],\n","       [6.8, 2.8, 4.8, 1.4],\n","       [6.7, 3. , 5. , 1.7],\n","       [6. , 2.9, 4.5, 1.5],\n","       [5.7, 2.6, 3.5, 1. ],\n","       [5.5, 2.4, 3.8, 1.1],\n","       [5.5, 2.4, 3.7, 1. ],\n","       [5.8, 2.7, 3.9, 1.2],\n","       [6. , 2.7, 5.1, 1.6],\n","       [5.4, 3. , 4.5, 1.5],\n","       [6. , 3.4, 4.5, 1.6],\n","       [6.7, 3.1, 4.7, 1.5],\n","       [6.3, 2.3, 4.4, 1.3],\n","       [5.6, 3. , 4.1, 1.3],\n","       [5.5, 2.5, 4. , 1.3],\n","       [5.5, 2.6, 4.4, 1.2],\n","       [6.1, 3. , 4.6, 1.4],\n","       [5.8, 2.6, 4. , 1.2],\n","       [5. , 2.3, 3.3, 1. ],\n","       [5.6, 2.7, 4.2, 1.3],\n","       [5.7, 3. , 4.2, 1.2],\n","       [5.7, 2.9, 4.2, 1.3],\n","       [6.2, 2.9, 4.3, 1.3],\n","       [5.1, 2.5, 3. , 1.1],\n","       [5.7, 2.8, 4.1, 1.3],\n","       [6.3, 3.3, 6. , 2.5],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [7.1, 3. , 5.9, 2.1],\n","       [6.3, 2.9, 5.6, 1.8],\n","       [6.5, 3. , 5.8, 2.2],\n","       [7.6, 3. , 6.6, 2.1],\n","       [4.9, 2.5, 4.5, 1.7],\n","       [7.3, 2.9, 6.3, 1.8],\n","       [6.7, 2.5, 5.8, 1.8],\n","       [7.2, 3.6, 6.1, 2.5],\n","       [6.5, 3.2, 5.1, 2. ],\n","       [6.4, 2.7, 5.3, 1.9],\n","       [6.8, 3. , 5.5, 2.1],\n","       [5.7, 2.5, 5. , 2. ],\n","       [5.8, 2.8, 5.1, 2.4],\n","       [6.4, 3.2, 5.3, 2.3],\n","       [6.5, 3. , 5.5, 1.8],\n","       [7.7, 3.8, 6.7, 2.2],\n","       [7.7, 2.6, 6.9, 2.3],\n","       [6. , 2.2, 5. , 1.5],\n","       [6.9, 3.2, 5.7, 2.3],\n","       [5.6, 2.8, 4.9, 2. ],\n","       [7.7, 2.8, 6.7, 2. ],\n","       [6.3, 2.7, 4.9, 1.8],\n","       [6.7, 3.3, 5.7, 2.1],\n","       [7.2, 3.2, 6. , 1.8],\n","       [6.2, 2.8, 4.8, 1.8],\n","       [6.1, 3. , 4.9, 1.8],\n","       [6.4, 2.8, 5.6, 2.1],\n","       [7.2, 3. , 5.8, 1.6],\n","       [7.4, 2.8, 6.1, 1.9],\n","       [7.9, 3.8, 6.4, 2. ],\n","       [6.4, 2.8, 5.6, 2.2],\n","       [6.3, 2.8, 5.1, 1.5],\n","       [6.1, 2.6, 5.6, 1.4],\n","       [7.7, 3. , 6.1, 2.3],\n","       [6.3, 3.4, 5.6, 2.4],\n","       [6.4, 3.1, 5.5, 1.8],\n","       [6. , 3. , 4.8, 1.8],\n","       [6.9, 3.1, 5.4, 2.1],\n","       [6.7, 3.1, 5.6, 2.4],\n","       [6.9, 3.1, 5.1, 2.3],\n","       [5.8, 2.7, 5.1, 1.9],\n","       [6.8, 3.2, 5.9, 2.3],\n","       [6.7, 3.3, 5.7, 2.5],\n","       [6.7, 3. , 5.2, 2.3],\n","       [6.3, 2.5, 5. , 1.9],\n","       [6.5, 3. , 5.2, 2. ],\n","       [6.2, 3.4, 5.4, 2.3],\n","       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']}\n"],"name":"stdout"}]},{"metadata":{"scrolled":true,"id":"F74Ur7a27CuO","colab_type":"code","colab":{}},"cell_type":"code","source":["print('\\n', iris.data.shape) ##데이터 개수와 특징 개수 확인\n","print('\\n \\n', iris.DESCR) ##Bunch안에 이 데이터가 어떤 데이터인지 알려주는 key와 value가 있음"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F3sI6_FL7CuU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"outputId":"1c26d21d-f07c-44a2-e051-bacafcea60a3","executionInfo":{"status":"ok","timestamp":1543032835493,"user_tz":0,"elapsed":1743,"user":{"displayName":"Ewan","photoUrl":"","userId":"17132486724952991358"}}},"cell_type":"code","source":["tree_clf = DecisionTreeClassifier(max_depth=2) \n","##사용하려는 알고리즘을 정의하고\n","tree_clf.fit(X, y) \n","##우리가 가진데이터를 집어 넣음"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n","            max_features=None, max_leaf_nodes=None,\n","            min_impurity_decrease=0.0, min_impurity_split=None,\n","            min_samples_leaf=1, min_samples_split=2,\n","            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n","            splitter='best')"]},"metadata":{"tags":[]},"execution_count":5}]},{"metadata":{"id":"aBkVSnlh7Cuc","colab_type":"text"},"cell_type":"markdown","source":["- Decision tree의 장점 중 하나는 훈련된 결정 트리를 시각화 할 수 있음"]},{"metadata":{"scrolled":true,"id":"jqFwYCmQ7Cue","colab_type":"code","colab":{}},"cell_type":"code","source":["##개인적으로 홈 디렉터리를 바꿔서 책에 있는 코드가 적용되지 않음\n","##따로 만듦\n","from sklearn.tree import export_graphviz\n","def export_graphviz_k2h(ML_algorithms, image_path_k2h=IMAGE_PATH):\n","    if not os.path.isdir(image_path_k2h):#github 코드로는 작동이 안되서\n","        os.makedirs(image_path_k2h) ##직접 image 폴더, decision_trees 폴더를 만듦\n","    export_graphviz(\n","    ML_algorithms,\n","    out_file=image_path(\"iris_tree.dot\"), ##C:\\Python\\image\\decisiontrees\\iris_tree.dot\n","    feature_names=iris.feature_names[2:],\n","    class_names=iris.target_names, \n","    rounded=True, ##박스 생김새\n","    filled=True ##박스 생김새\n","        )\n","    \n","export_graphviz_k2h(tree_clf) ##dot 파일을 만들어줌"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"FXennv957Cuj","colab_type":"code","colab":{}},"cell_type":"code","source":["help(export_graphviz)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HA7slXvI7Cur","colab_type":"text"},"cell_type":"markdown","source":["graphviz 프로그램을 따로 다운로드 해줘야함."]},{"metadata":{"id":"lyLnbD4x7Cut","colab_type":"text"},"cell_type":"markdown","source":["책에서는 명령 프롬프트에서  dot -Tpng iris_tree.dot -o iris_tree.png를 실행해서 png 파일을 만들지만,  \n","\n","github에서 쥬피터 노트북에서 png파일 바로 볼 수 있는 코드를 알려줌."]},{"metadata":{"id":"nEbsf_jj7Cuv","colab_type":"code","colab":{}},"cell_type":"code","source":["import graphviz\n","with open(\"images/decision_trees/iris_tree.dot\") as f:\n","    dot_graph = f.read()\n","\n","dot = graphviz.Source(dot_graph)\n","dot.format = 'png'\n","dot.render(filename='iris_tree', directory='images/decision_trees', cleanup=True)\n","dot ##시스템 변수, PATH에 graphviz가 있어야함"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3dsN_7Nz7Cu3","colab_type":"text"},"cell_type":"markdown","source":["***"]},{"metadata":{"id":"uLVEqIMU7Cu5","colab_type":"text"},"cell_type":"markdown","source":["## 6.2. Making Predictions"]},{"metadata":{"id":"XeVs9GqC7Cu6","colab_type":"text"},"cell_type":"markdown","source":["용어정리  \n","- root node: 깊이가 0인 맨 꼭대기의 노드\n","- child node: 한 노드에서 나오는 다른 노드\n","- leaf node: child node를 가지지 않는 노드"]},{"metadata":{"id":"cR0dyaFR7Cu8","colab_type":"text"},"cell_type":"markdown","source":["예측 방식  \n","petal length가 2.45cm보다 긴 새로운 꽃을 발견했다고 하자.  \n","- root node의 오른쪽 child node로 이동\n","- 오른쪽 child는 leaf node가 아니여서, 추가로 petal width를 검사\n","- 왼쪽 혹은 오른쪽 leaf node로 이동  \n","\n","- 예측 완료"]},{"metadata":{"id":"wx5E3lAR7Cu9","colab_type":"text"},"cell_type":"markdown","source":["Note. 결정 트리의 여러 장점 중 하나는 데이터 전처리가 거의 필요하지 않다는 것이다. 특히 특성의 스케일을 맞추거나 평균을 원점에 맞추는 작업이 필요하지 않다."]},{"metadata":{"id":"13xkps-Y7CvA","colab_type":"code","colab":{}},"cell_type":"code","source":["import graphviz\n","with open(\"images/decision_trees/iris_tree.dot\") as f:\n","    dot_graph = f.read()\n","\n","dot = graphviz.Source(dot_graph)\n","dot.format = 'png'\n","dot.render(filename='iris_tree', directory='images/decision_trees', cleanup=True)\n","dot ##시스템 변수, PATH에 graphviz가 있어야함"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7W7z3ENq7CvH","colab_type":"text"},"cell_type":"markdown","source":["박스 내부 용어 정리  \n","\n","sample - 적용된 훈련 샘플의 개수  \n","value - 그 노드에서 각 class에 있는 훈련 샘플의 개수  \n","gini - 불순도 (gini는 불순도 측정 방식 중 하나, 엔트로피를 쓸 때도 있음, 엔트로피 방정식은 뒤에서 설명)"]},{"metadata":{"id":"6evav4S67CvK","colab_type":"text"},"cell_type":"markdown","source":["- 지니계수\n","$$G_i = \\sum^n_{k=1} p_{i,k} (1 - p_{i,k}) = 1 - \\sum^n_{k=1} p^2_{i,k}$$  \n","where $p_{i,k}$ is the ratio of class $k$ instances among the training instance in the $i^{th}$ node."]},{"metadata":{"id":"ZUoJY7V47CvN","colab_type":"text"},"cell_type":"markdown","source":["질문.  \n","\n","지니 계수는 불순도를 잘 나타내주는 지표인가?\n","\n","\n","> 이 질문을 해결하려면 불순도 측정 방식을 잘 정의하고(metric처럼), 그 특성을 가지고 지니 계수가 불순도를 측정하는 방식에 부합하는지를 살펴봐야 할 듯."]},{"metadata":{"id":"E2BRz9zy7CvS","colab_type":"text"},"cell_type":"markdown","source":["Note. sklearn은 __이진 트리__ 만 만드는 CART알고리즘을 사용(CART알고리즘은 뒤에서 추가로 설명)  \n","따라서, leaf node를 제외한 모든 node는 child node를 두 개씩 가짐"]},{"metadata":{"scrolled":true,"id":"ykkU0t3F7CvT","colab_type":"code","colab":{}},"cell_type":"code","source":["from matplotlib.colors import ListedColormap\n","\n","def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n","    x1s = np.linspace(axes[0], axes[1], 100)\n","    x2s = np.linspace(axes[2], axes[3], 100)\n","    x1, x2 = np.meshgrid(x1s, x2s)\n","    X_new = np.c_[x1.ravel(), x2.ravel()]\n","    y_pred = clf.predict(X_new).reshape(x1.shape)\n","    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n","    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n","    if not iris:\n","        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n","        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n","    if plot_training:\n","        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\n","        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\n","        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\")\n","        plt.axis(axes)\n","    if iris:\n","        plt.xlabel(\"꽃잎 길이\", fontsize=14)\n","        plt.ylabel(\"꽃잎 너비\", fontsize=14)\n","    else:\n","        plt.xlabel(r\"$x_1$\", fontsize=18)\n","        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n","    if legend:\n","        plt.legend(loc=\"lower right\", fontsize=14)\n","\n","plt.figure(figsize=(8, 4))\n","plot_decision_boundary(tree_clf, X, y)\n","plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)\n","plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)\n","plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)\n","plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)\n","plt.text(1.40, 1.0, \"깊이=0\", fontsize=15)\n","plt.text(3.2, 1.80, \"깊이=1\", fontsize=13)\n","plt.text(4.05, 0.5, \"(깊이=2)\", fontsize=11)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jfoLoSUh7Cvd","colab_type":"text"},"cell_type":"markdown","source":["- max_depth를 0,1,2로 설정함에 따라 결정 경계가 추가로 생김"]},{"metadata":{"id":"H-tzgoys7Cvf","colab_type":"text"},"cell_type":"markdown","source":["***"]},{"metadata":{"id":"JFk6iTOi7Cvg","colab_type":"text"},"cell_type":"markdown","source":["## 6.3 Estimating Class"]},{"metadata":{"id":"KoQRxmqF7Cvh","colab_type":"text"},"cell_type":"markdown","source":["- 결정 트리는 한 샘플이 특정 클래스 k에 속할 확률을 추정할 수도 있음\n","- 샘플을 알고리즘에 적용하면 leaf node를 찾고, 그 노드에 있는 클래스 k의 훈련 샘플의 비율을 반환\n","- 클래스를 하나 예측한다면 (tree_clf.predict으로 예측 값을 출력한다면), 가장 높은 확률을 가진 클래스를 출력"]},{"metadata":{"id":"Qx5inYvw7Cvk","colab_type":"code","colab":{}},"cell_type":"code","source":["tree_clf.predict_proba([[5, 1.5]])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u5xH8e0A7Cvp","colab_type":"code","colab":{}},"cell_type":"code","source":["tree_clf.predict([[5, 1.5]])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"N0EUMuOP7Cvv","colab_type":"text"},"cell_type":"markdown","source":["***"]},{"metadata":{"id":"P1Cg_ZON7Cvw","colab_type":"text"},"cell_type":"markdown","source":["## 6.4 The CART Training Algorithm"]},{"metadata":{"id":"wlG0NVy-7Cvy","colab_type":"text"},"cell_type":"markdown","source":["- Classification And Regression Tree Algorithm의 약자\n","- 먼저 훈련 세트를 하나의 특성 $k$의 임곗값 $t_k$를 사용해 두 개의 서브셋으로 나눔"]},{"metadata":{"id":"BHlqTI4T7Cvz","colab_type":"text"},"cell_type":"markdown","source":["- $t$와 $t_k$를 고르는 방식\n","> 가장 순수한(불순도가 낮은) 서브셋으로 나눌 수 있는 ($k$,$t_k$) 짝을 찾음\n","\n","\n","\n","\n","$$J_{k, t_k} = \\frac{m_{left}}{m} G_{left} + \\frac{m_{right}}{m} G_{right}$$\n","where $G_{left/right}$ measures the impurity of the left/right subset, and $m_{left/right}$ is the number of instances in the left/right subset. "]},{"metadata":{"id":"tL_Bkw4l7Cv1","colab_type":"text"},"cell_type":"markdown","source":["Note. Gain함수: 특성과 임계값에 따라 얻을 수 있는 정보량  \n","$$ Gain(k) = Entropy(X) - J_{k,t_k} $$\n","gain함수가 높은 특성 k를 구해야함"]},{"metadata":{"id":"AtrexwUT7Cv2","colab_type":"text"},"cell_type":"markdown","source":["라온 피플에서 말한 entropy에 대한 말은 아마 gain함수와 entropy를 혼용해서 쓴 것 같음"]},{"metadata":{"id":"ax1totmnFAWv","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"a6r_nc-O7Cv3","colab_type":"text"},"cell_type":"markdown","source":["질문.\n","\n","이러한 비용 함수가 합리적인가?  \n","특성이 범주형 자료가 아니고 연속적인 숫자형 자료일 때 임곗값을 구체적으로 정하는 방법?  \n","연속적인 숫자형 자료라면 임곗값을 옮기는 step(?)은 어떻게 설정하는지?"]},{"metadata":{"id":"sjKLnmYT7Cv5","colab_type":"text"},"cell_type":"markdown","source":["- 이와 같은 방식으로 서브셋의 서브셋을 나눔\n","- 이 과정은 max_depth가 되면 중지하거나 불순도를 줄이는 분할을 찾을 수 없을 때 멈추게 됨\n","- 6.7에서 배우는 매개변수도 중지 조건에 관여하게 됨\n","\n","\n","- 이 알고리즘은 각 단계에서 최적의 분할을 찾기 때문에 현재 단계의 분할이 몇 단계를 거쳐 가장 낮은 불순도로 이어질 수 있을 지는 고려하지 않음\n","- 다시 말해, 최적의 솔루션은 보장하지 못함"]},{"metadata":{"id":"jQpYw3nD7Cv7","colab_type":"text"},"cell_type":"markdown","source":["***"]},{"metadata":{"id":"3fC4oNFP7Cv9","colab_type":"text"},"cell_type":"markdown","source":["## 6.5 Computational Complexity"]},{"metadata":{"id":"e0AydJz37Cv-","colab_type":"text"},"cell_type":"markdown","source":["- 예측에 필요한 전체 복잡도는 특성 수와 무관하게 $O(\\log_2(m))$\n","- 그러나 훈련 알고리즘은 각 노드에서 모든 훈련 샘플의 모든 특성을 비교하기 때문에, 훈련 복잡도는 $O(n \\times m \\log(m))$\n","\n","\n","- 여기서 $O(m\\log(m))$은 데이터들을 정렬하는데 걸리는 계산 복잡도"]},{"metadata":{"id":"GgVWQjnE7CwA","colab_type":"text"},"cell_type":"markdown","source":["***"]},{"metadata":{"id":"HOx-eg3P7CwB","colab_type":"text"},"cell_type":"markdown","source":["## 6.6 Gini Impurity or Entropy"]},{"metadata":{"id":"XMdlp4jP7CwC","colab_type":"text"},"cell_type":"markdown","source":["- criterion 매개변수를 'entropy'로 지정하면 엔트로피 불순도를 사용할 수 있음\n","- 엔트로피 방정식\n","$$ H_i = - \\sum^n_{k=1 \\\\ P_{i,k}} P_{i,k}\\log_2(P_{i,k}) $$\n","\n","\n","- 실제로는 지니 불순도와 큰 차이는 없음\n","- 지니 불순도가 조금 더 계산이 빠름\n","- 지니 불순도는 가장 빈도 높은 클래스를 한쪽 가지로 고립시키는 경향이 있는 반면 엔트로피는 조금 더 균형 잡힌 트리를 만듦"]},{"metadata":{"id":"2zFimSOR7CwH","colab_type":"text"},"cell_type":"markdown","source":["질문.\n","\n","엔트로피는 불순도를 잘 나타내주는 지표인가?\n","\n","이 질문을 해결하려면 불순도 측정 방식을 잘 정의하고 (metric처럼), 그 특성을 가지고 엔트로피가 불순도를 측정하는 방식에 부합하는 지를 살펴봐야 할 듯."]},{"metadata":{"id":"CmrvSWjb7CwI","colab_type":"text"},"cell_type":"markdown","source":["***"]},{"metadata":{"id":"ItD4B1Qh7CwK","colab_type":"text"},"cell_type":"markdown","source":["## 6.7 Regulariztion Hyperparameters"]},{"metadata":{"id":"dBULg0Er7CwL","colab_type":"text"},"cell_type":"markdown","source":["- 결정 트리는 훈련 데이터에 대한 제약사항이 거의 없음\n","- 따라서 제한을 두지 않으면 트리가 훈련 데이터에 아주 가깝게 맞추려고 해서 대부분 과대적합이 되기 쉬움\n","- 트리의 자유도를 제한해서 문제를 해결"]},{"metadata":{"id":"x-3i4b_K7CwM","colab_type":"text"},"cell_type":"markdown","source":["- min_samples_split - 분할 되기 위해 노드가 가져야 하는 최소 샘플 수\n","- min_samples_leaf - 리프 노드가 가지고 있어야 할 최소 샘플 수\n","- min_weight_fraction_leaf - 가중치가 부여된 전체 샘플 수에서의 비율(?)"]},{"metadata":{"id":"BUN9V6Ag7CwO","colab_type":"text"},"cell_type":"markdown","source":["- max_depth - 최대 깊이\n","- max_leaf_nodes - 리프 노드의 최대 수\n","- max_features - 각 노드에서 분할에 사용할 특성의 최대 수"]},{"metadata":{"id":"V0SKRoKc7CwR","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.datasets import make_moons\n","Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)\n","\n","deep_tree_clf1 = DecisionTreeClassifier(random_state=42)\n","deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n","deep_tree_clf1.fit(Xm, ym)\n","deep_tree_clf2.fit(Xm, ym)\n","\n","plt.figure(figsize=(11, 4))\n","plt.subplot(121)\n","plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n","plt.title(\"규제 없음\", fontsize=16)\n","plt.subplot(122)\n","plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n","plt.title(\"min_samples_leaf = {}\".format(deep_tree_clf2.min_samples_leaf), fontsize=14)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mFzjkg0K7CwW","colab_type":"text"},"cell_type":"markdown","source":["***"]},{"metadata":{"id":"KsXcWGBd7CwX","colab_type":"text"},"cell_type":"markdown","source":["## 6.8. Regression"]},{"metadata":{"id":"GTFran0x7CwZ","colab_type":"code","colab":{}},"cell_type":"code","source":["np.random.seed(42)\n","m = 200\n","X = np.random.rand(m, 1)\n","y = 4 * (X - 0.5) ** 2\n","y = y + np.random.randn(m, 1) / 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5MmIh81m7Cwg","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","\n","tree_reg = DecisionTreeRegressor(max_depth=1, random_state=42)\n","tree_reg.fit(X, y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"05o_hyer7Cwp","colab_type":"code","colab":{}},"cell_type":"code","source":["tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n","tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\n","tree_reg1.fit(X, y)\n","tree_reg2.fit(X, y)\n","\n","def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n","    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n","    y_pred = tree_reg.predict(x1)\n","    plt.axis(axes)\n","    plt.xlabel(\"$x_1$\", fontsize=18)\n","    if ylabel:\n","        plt.ylabel(ylabel, fontsize=18, rotation=0)\n","    plt.plot(X, y, \"b.\")\n","    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n","\n","plt.figure(figsize=(11, 4))\n","plt.subplot(121)\n","plot_regression_predictions(tree_reg1, X, y)\n","for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n","    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n","plt.text(0.21, 0.65, \"깊이=0\", fontsize=15)\n","plt.text(0.01, 0.2, \"깊이=1\", fontsize=13)\n","plt.text(0.65, 0.8, \"깊이=1\", fontsize=13)\n","plt.legend(loc=\"upper center\", fontsize=18)\n","plt.title(\"max_depth=2\", fontsize=14)\n","\n","plt.subplot(122)\n","plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n","for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n","    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n","for split in (0.0458, 0.1298, 0.2873, 0.9040):\n","    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n","plt.text(0.3, 0.5, \"깊이=2\", fontsize=13)\n","plt.title(\"max_depth=3\", fontsize=14)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-LVEopmV7Cwx","colab_type":"code","colab":{}},"cell_type":"code","source":["def export_graphviz_k2h(ML_algorithms, image_path_k2h=IMAGE_PATH):\n","    if not os.path.isdir(image_path_k2h):#github 코드로는 작동이 안되서\n","        os.makedirs(image_path_k2h) ##직접 image 폴더, decision_trees 폴더를 만듦\n","    export_graphviz(\n","    ML_algorithms,\n","    out_file=image_path(\"regression_tree.dot\"), ##C:\\Python\\image\\decisiontrees\\iris_tree.dot\n","    feature_names=[\"x1\"], \n","    rounded=True, ##박스 생김새\n","    filled=True ##박스 생김새\n","        )\n","    \n","export_graphviz_k2h(tree_reg) ##dot 파일을 만들어줌"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rlzyxohh7Cw3","colab_type":"code","colab":{}},"cell_type":"code","source":["import graphviz\n","with open(\"images/decision_trees/regression_tree.dot\") as f:\n","    dot_graph = f.read()\n","\n","dot = graphviz.Source(dot_graph)\n","dot.format = 'png'\n","dot.render(filename='regression_tree', directory='images/decision_trees', cleanup=True)\n","dot ##시스템 변수, PATH에 graphviz가 있어야함"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hnOzmQDk7CxC","colab_type":"code","colab":{}},"cell_type":"code","source":["np.mean(y)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9nt0C6w77CxO","colab_type":"text"},"cell_type":"markdown","source":["- 앞서 만든 분류 트리와 비슷해 보이지만, 주요한 차이는 각 노드에서 클래스를 예측하는 대신 어떤 값을 예측한다는 것이다.  \n","- 만약 \\$x_1$ = 0.6인 샘플을 넣게 되면, value=0.1106인 leaf node에 도달하게 된다.  \n","- leaf node에 있는 110개 훈련 샘플의 평균 타깃값으로 예측된다.  \n","- 이 예측값을 사용해 110개 샘플에 대한 평균제곱오차(MSE)를 계산하면 0.0151이 된다."]},{"metadata":{"id":"uMFHHzxL7CxV","colab_type":"text"},"cell_type":"markdown","source":["- 회귀를 위한 CART 비용함수\n","\n","$$ J(k,t_k) = \\frac{m_{left}}{m}MSE_{left} + \\frac{m_{right}}{m}MSE_{right} \\\\ $$\n","\n","$$\\text{where}\\ \\   MSE_{node} = \\sum_{i \\in node} (\\hat{y}_{node} - y^{(i)})^2 \\\\ \n","\\hat{y}_{node} = \\frac{1}{m_{node}} \\sum_{i \\in node}y^{(i)} $$"]},{"metadata":{"id":"c4YOFBXc7CxY","colab_type":"text"},"cell_type":"markdown","source":["- 회귀 작업에서도 결정 트리가 과대적합되기 쉬움"]},{"metadata":{"id":"Oa9tXFNX7CxZ","colab_type":"text"},"cell_type":"markdown","source":["***"]},{"metadata":{"id":"K64SaFpR7Cxb","colab_type":"text"},"cell_type":"markdown","source":["## 6.9 Instability"]},{"metadata":{"id":"kAZeY3dD7Cxd","colab_type":"text"},"cell_type":"markdown","source":["- 결정 트리는 이해하고 해석하기 쉽고, 사용하기 편하고, 여러 용도로 사용할 수 있으며, 성능도 뛰어나다.\n","\n","\n","- 하지만 결정 트리는 계단 모향의 결정 경계를 만든다. 다시 말해, 축에 수직인 경계를 만든다. 따라서 회전에 민감하다.\n","\n","- 이런 문제를 해결하는 한 가지 방법은 훈련 데이터를 더 좋은 방향으로 회전시키는 PCA 기법을 사용하는 것이다.\n","\n","\n","- 훈련 데이터에 있는 작은 변화에도 민감하다.\n","- 사이킷런에서 사용하고 있는 훈련 알고리즘은 확률적이기 때문에 같은 훈련 데이터에서도 다른 모델을 얻게 될 수 있다.\n","- 각 노드에서 평가할 후보 특성을 무작위로 선택\n","- 다음 장에서 보게 될 랜덤 포레스트는 많은 트리에서 만든 예측을 평균하기 때문에 이런 불안정성을 극복할 수 있음"]},{"metadata":{"id":"hxpfmICh7Cxf","colab_type":"text"},"cell_type":"markdown","source":["Note. 매개변수에서 분할에 사용할 특성의 최대 개수를 지정할 수 있는데, 데이터셋의 특성 개수보다 작게 설정하면 무작위로 일부 특성이 선택된다."]}]}