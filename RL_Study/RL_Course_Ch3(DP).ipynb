{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fundamental of Reinforcement Learning(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 다이내믹 프로그래밍(Dynamic programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다이내믹 프로그래밍은 작은 문제가 큰 문제 안에 중첩돼 있는 경우에 작은 문제의 답을 다른 작은 문제에 이용함으로써 효율적으로 계산하는 방법이다. 벨만(Bellman)이 만든 다이내믹 프로그래밍은 후에 강화학습의 근간이 된다. 다이내믹 프로그래밍의 한계를 극복하려고 나온게 강화학습이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 순차적 행동 결정 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화 학습은 순차적 행동 결정 문제를 푸는 방법 중 하나다. 앞서 배운 벨만 방정식을 이용해 순차적 행동 결정 문제를 푸는 방법을 정리하면 다음과 같다.\n",
    "1. 순차적 행동 문제를 MDP로 전환\n",
    "2. 가치함수를 벨만 방정식으로 반복적 계산\n",
    "3. 최적 가치함수와 최적 정책을 찾기\n",
    "<br><br>\n",
    "1번에 대해 했으니 이번에는 2,3번에 대해 알아보겠다.\n",
    "<br><br>\n",
    "\n",
    "벨만 방정식을 푼다는 것은\n",
    "$$v_*(s) = \\max_{a}E_\\pi[R_{t+1} + \\gamma v_*(S_{t+1})|S_t = s, A_t = a]$$\n",
    "을 만족하는 $v_*(s)$를 구하겠다는 뜻이다. 이는 큐함수에서도 마찬가지이다.\n",
    "<br><br>\n",
    "\n",
    "이 벨만 방정식을 계산하고 푸는 것이 바로 2, 3번에서 하고자 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 다이내믹 프로그래밍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순차적 행동 문제를 푸는 방법은 여러가지가 있다. 그중에 하나가 바로 강화학습이다. 그런데 강화학습 이전에 벨만 방정식을 푸는 알고리즘이 존재 했는데 그것이 바로 이 다이내믹 프로그래밍이다. 여기서 프로그래밍이란 '컴퓨터 프로그래밍'을 뜻하는 것이 아니라 계획을 하는 것으로 여러 프로세스가 다단계로 이뤄진다는 것을 의미한다.\n",
    "<br><br>\n",
    "\n",
    "다이내믹 프로그래밍의 기본 아이디어는 작은 문제들이 중첩된 큰 문제를 그 작은 문제들로 쪼개서 풀겠다는 것이다. 여기서 작은 문제들이 하나의 프로세스가 되는 것이고 이것을 다단계로 풀어나가는 것이 프로그래밍이 된다. 하나의 프로세스를 대상으로 푸는 것이 아니라 시간에 따라 다른 프로세스들을 풀어나가기 때문에 다이내믹 프로그래밍이라 부른다.\n",
    "<br><br>\n",
    "\n",
    "각각의 작은 문제들은 서로 연관되어있기 때문에 해답을 서로 이용할 수 있다. 이 특성을 사용해 계산량을 줄인다. \n",
    "<br><br>\n",
    "\n",
    "예를 들어 세 개의 상태가 있다고 가정하겠다. 문제의 목표는 각 상태의 참 가치함수($v_\\pi(s_1)$,$v_\\pi(s_2)$,$v_\\pi(s_3)$)를 구하는 것이고 이것이 하나의 큰 문제이다. 이때 작은 문제로 쪼개서 풀겠다는 것은 저 문제를 한번에 푸는 것이 아니라 \n",
    "$$v_0(s)\\to v_1(s)\\to \\cdots \\to v_k(s)\\to \\cdots \\to v_\\pi(s)$$\n",
    "<br>\n",
    "위와 같이 반복적인 계산을 통해 풀겠다는 것이다. 이 계산은 모든 상태에 대해 한 번 계산이 끝나면 모든 상태의 가치함수를 업데이트 한다. 다음 계산은 업데이트된 가치함수를 이용해 같은 과정을 반복한다. 이런 방식으로 효율적인 계산이 이루어 진다.\n",
    "<br><br>\n",
    "\n",
    "다이내믹 프로그래밍에는 정책 이터레이션(Policy iteration)과 가치 이터레이션(Value iteration)이 있다. 정책 이터레이션은 벨만 기대 방정식을, 가치 이터레이션은 벨만 최적 방정식을 푸는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 격자로 이뤄진 간단한 예제: 그리드 월드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP에서 했던 그리드 월드 예제를 그대로 다이나믹 프로그래밍의 예시로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 다이내믹 프로그래밍1: 정책 이터레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 강화학습 알고리즘의 흐름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL알고리즘 흐름도](.\\image\\RL_algorithm_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP로 정의되는 문제의 목표는 Agent가 받는 총 보상을 최대로 하는 것이다. Agent는 가치 함수를 통해 이 목표에 얼마나 다가갔는지 알 수 있다. 이 가치함수에 대한 방정식이 벨만 방정식이다. \n",
    "<br><br>\n",
    "\n",
    "정책 이터레이션과 가치 이터레이션은 살사(SARSA)로 발전하고, 살사는 오프폴리시(Off-policy) 방법으로 변형되 큐러닝(Q-learning)으로 이어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 정책 이터레이션(Policy iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 이 문제에서 알고 싶은 것은 가장 높은 보상을 얻게하는 정책이다. 처음에는 이 정책을 알 수 없으니 우리는 어떤 특정한 정책으로 부터 발전시켜나가야 한다. 보통 무작위로 행동하는 정책(Random policy)으로 시작한다. 정책 이터레이션에서는 정책을 평가(Policy evaluation)하고 정책을 발전(Policy improvement)한다. 이 과정을 반복하면 정책은 최적의 형태로 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 정책 평가(Policy evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 평가는 가치 함수로 한다. 그래서 사용하는 것이 바로 벨만 기대 방정식이다.\n",
    "$$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$$\n",
    "<br>\n",
    "이를 계산 가능한 형태로 바꾸면\n",
    "$$v_\\pi(s)=\\sum_{a \\in A} \\pi (a|s)(R_{t+1} + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_\\pi (s'))$$\n",
    "이 된다. 하지만 그리드 월드에서는 상태 변환 확률을 1이라고 가정하기 때문에 식은 다음과 같이 된다.\n",
    "$$v_\\pi(s)=\\sum_{a \\in A} \\pi (a|s)(R_{t+1} + \\gamma v_\\pi (s'))$$\n",
    "<br>\n",
    "정책 평가는 $\\pi$라는 정책에 대해 반복적으로 수행하는 것이다. 따라서 계산 단계를 $k$ $(k=1,2,3,\\dots)$라고 할 때 $k$번째 가치함수를 통해 $k+1$번째 가치함수를 계산하는 방정식은 다음과 같다.\n",
    "$$v_{k+1}(s)=\\sum_{a \\in A} \\pi (a|s)(R_{t+1} + \\gamma v_k (s'))$$\n",
    "<br><br>\n",
    "\n",
    "한 번의 정책 평가 과정을 순서대로 나타내면 다음과 같다.\n",
    "1. $k$번째 가치함수 행렬에서 현재 상태 $s$에서 갈 수 있는 다음 상태 $s'$에 저장되어 있는 가치 함수 $v_k(s')$을 불러온다.\n",
    "2. $v_k(s')$에 감가율 $\\gamma$를 곱하고 그 상태로 가능 행동에 대한 보상 $R_s^a$를 더한다.\n",
    "$$R_s^a + v_k(s')$$\n",
    "3. 2번에서 구한 값에 정책 값을 곱한다.\n",
    "$$\\pi(a|s)(R_s^a + \\gamma v_k(s'))$$\n",
    "4. 3번을 모든 선택가능한 행동에 대해 반복하고 합친다.\n",
    "$$\\sum_{a \\in A} \\pi (a|s)(R_s^a + \\gamma v_k (s'))$$\n",
    "5. 4번의 값을 $k+1$번째 가치 함수 행렬의 상태 $s$자리에 저장한다.\n",
    "6. 1-5의 과정을 모든 $s \\in S$에 대해 반복한다.\n",
    "<br><br>\n",
    "\n",
    "이를 무한번 반복하면 참 $v_\\pi$를 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 정책 발전(Policy improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책을 평가를 했으면 발전을 시켜야 된다. 여러 방법 중에 탐욕 정책 발전(Greedy policy improvement)이라는 방법이 있다. 아이디어는 다음 상태 중에 가장 가치 함수가 높은 곳으로 가겠다는 것이다.\n",
    "<br><br>\n",
    "\n",
    "정책 평가를 통해 Agent가 정책을 따랏을 때의 모든 상태에 대한 가치를 구했다면, 이제 큐함수를 통해 어떤 행동이 좋은지 알아낼 수 있다. 계산 가능한 형태의 큐함수는 다음과 같다.\n",
    "\n",
    "$$q_\\pi(s,a) = R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_\\pi (s')$$\n",
    "<br>\n",
    "여기서 Agent가 할 일은 선택 가능한 행동의 $q_\\pi(s,a)$중에서 max값을 가지는 행동을 취하면 된다. 이것이 탐욕 정책 발전이다. 수식으로 표현하면 다음과 같다.\n",
    "$$\\pi'(s) = \\def\\argmax{\\text{argmax}} \\argmax_{a\\in A} q_\\pi(s,a)$$\n",
    "<br><br>\n",
    "\n",
    "이제 이 새롭게 업데이트된 정책으로 다시 정책 평가가 들어가고 정책 발전이 이루어진다. 이를 반복하므로써 기존의 정책은 최적의 정책으로 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.5 정책 이터레이션 코드 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "from environment.env_policy import GraphicDisplay, Env\n",
    "\n",
    "\n",
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n",
    "        # 환경에 대한 객체 선언\n",
    "        self.env = env\n",
    "        # 가치함수를 2차원 리스트로 초기화\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # 상 하 좌 우 동일한 확률로 정책 초기화\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n",
    "                                    for _ in range(env.height)]\n",
    "        # 마침 상태의 설정\n",
    "        self.policy_table[2][2] = []\n",
    "        # 감가율\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "\n",
    "        # 다음 가치함수 초기화\n",
    "        next_value_table = [[0.00] * self.env.width\n",
    "                                    for _ in range(self.env.height)]\n",
    "\n",
    "        # 모든 상태에 대해서 벨만 기대방정식을 계산\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            # 마침 상태의 가치 함수 = 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            # 벨만 기대 방정식\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "\n",
    "            next_value_table[state[0]][state[1]] = round(value, 2)\n",
    "\n",
    "        self.value_table = next_value_table\n",
    "\n",
    "    # 현재 가치 함수에 대해서 탐욕 정책 발전\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_table\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                continue\n",
    "            value = -99999\n",
    "            max_index = []\n",
    "            # 반환할 정책 초기화\n",
    "            result = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "            # 모든 행동에 대해서 [보상 + (감가율 * 다음 상태 가치함수)] 계산\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                temp = reward + self.discount_factor * next_value\n",
    "\n",
    "                # 받을 보상이 최대인 행동의 index(최대가 복수라면 모두)를 추출\n",
    "                if temp == value:\n",
    "                    max_index.append(index)\n",
    "                elif temp > value:\n",
    "                    value = temp\n",
    "                    max_index.clear()\n",
    "                    max_index.append(index)\n",
    "\n",
    "            # 행동의 확률 계산\n",
    "            prob = 1 / len(max_index)\n",
    "\n",
    "            for index in max_index:\n",
    "                result[index] = prob\n",
    "\n",
    "            next_policy[state[0]][state[1]] = result\n",
    "\n",
    "        self.policy_table = next_policy\n",
    "\n",
    "    # 특정 상태에서 정책에 따른 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        # 0 ~ 1 사이의 값을 무작위로 추출\n",
    "        random_pick = random.randrange(100) / 100\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        policy_sum = 0.0\n",
    "        # 정책에 담긴 행동 중에 무작위로 한 행동을 추출\n",
    "        for index, value in enumerate(policy):\n",
    "            policy_sum += value\n",
    "            if random_pick < policy_sum:\n",
    "                return index\n",
    "\n",
    "    # 상태에 따른 정책 반환\n",
    "    def get_policy(self, state):\n",
    "        if state == [2, 2]:\n",
    "            return 0.0\n",
    "        return self.policy_table[state[0]][state[1]]\n",
    "\n",
    "    # 가치 함수의 값을 반환\n",
    "    def get_value(self, state):\n",
    "        # 소숫점 둘째 자리까지만 계산\n",
    "        return round(self.value_table[state[0]][state[1]], 2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    policy_iteration = PolicyIteration(env)\n",
    "    grid_world = GraphicDisplay(policy_iteration)\n",
    "    grid_world.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 env 객체에 정의돼 있는 변수와 함수는 다음과 같다.\n",
    "- env.width, env.height : 그리드월드의 너비와 높이. \n",
    "<br>그리드월드의 가로새로를 정수로 반환\n",
    "- env.state_after_action(state, action) : 특정 상태에서 특정 행동을 했을 때 에이전트가 가는 다음 상태\n",
    "<br>행동 후의 상태를 좌표로 표현한 리스트를 반환 ex) [1,2]\n",
    "- env.get_all_states() : 존재하는 모든 상태\n",
    "<br>모든 상태를 반환 ex)[[0,0],[0,1], ... , [4,4]]\n",
    "- env.get_reward(state) : 특정 상태의 보상\n",
    "<br>정수의 형태로 보상을 반환\n",
    "- env.possible_actions : 상, 하, 좌, 우\n",
    "<br>[0,1,2,3]을 반환, 순서대로 상하좌우를 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 평가에서 Agent는 모든 상태의 가치함수를 업데이트 한다. 가치함수를 업데이트 하기 위해 next_value_table을 선언한 다음 모든 상태에 대해 벨만 기대 방정식 계산이 끝나면 value_table에 next_value_table을 업데이트 한다. \n",
    "<br><br>\n",
    "정책 평가에 사용하는 벨만 기대 방정식은 다음과 같다.\n",
    "$$v_{k+1}(s)=\\sum_{a \\in A} \\pi (a|s)(R_s^a + \\gamma v_k (s'))$$\n",
    "<br>\n",
    "식에서 이를 구현한 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value += (self.get_policy(state)[action] * (reward + self.discount_factor * next_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy_imporvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 평가를 통해 정책이 평가되면 그에 따른 새로운 가치함수를 얻는다. Agent는 그 새로운 가치함수를 통해 정책을 업데이트 한다. 정책 평가와 마찬가지로 policy_table을 복사한 next_policy에 업데이트 된 정책을 저장한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(self):\n",
    "    next_policy = self.policy_table\n",
    "    for state in self.env.get_all_states():\n",
    "        if state == [2, 2]:\n",
    "            continue\n",
    "        value = -99999\n",
    "        max_index = []\n",
    "        # 반환할 정책 초기화\n",
    "        result = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "        # 모든 행동에 대해서 [보상 + (감가율 * 다음 상태 가치함수)] 계산\n",
    "        for index, action in enumerate(self.env.possible_actions):\n",
    "            next_state = self.env.state_after_action(state, action)\n",
    "            reward = self.env.get_reward(state, action)\n",
    "            next_value = self.get_value(next_state)\n",
    "            temp = reward + self.discount_factor * next_value\n",
    "\n",
    "            # 받을 보상이 최대인 행동의 index(최대가 복수라면 모두)를 추출\n",
    "            if temp == value:\n",
    "                max_index.append(index)\n",
    "            elif temp > value:\n",
    "                value = temp\n",
    "                max_index.clear()\n",
    "                max_index.append(index)\n",
    "\n",
    "        # 행동의 확률 계산\n",
    "        prob = 1 / len(max_index)\n",
    "\n",
    "        for index in max_index:\n",
    "            result[index] = prob\n",
    "\n",
    "        next_policy[state[0]][state[1]] = result\n",
    "\n",
    "    self.policy_table = next_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 발전에서는 다음의 식을 구해서 정책에 업데이트 하는 것이 목적이다.\n",
    "$$\\pi'(s) = \\def\\argmax{\\text{argmax}} \\argmax_{a\\in A} q_\\pi(s,a)$$\n",
    "\n",
    "따라서 현재 상태에서 가능한 행동에 대해 $R + \\gamma v(s')$를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # 모든 행동에 대해서 [보상 + (감가율 * 다음 상태 가치함수)] 계산\n",
    "        for index, action in enumerate(self.env.possible_actions):\n",
    "            next_state = self.env.state_after_action(state, action)\n",
    "            reward = self.env.get_reward(state, action)\n",
    "            next_value = self.get_value(next_state)\n",
    "            temp = reward + self.discount_factor * next_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 가장 높은 값의 인덱스를 max_index에 담는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # 받을 보상이 최대인 행동의 index(최대가 복수라면 모두)를 추출\n",
    "            if temp == value:\n",
    "                max_index.append(index)\n",
    "            elif temp > value:\n",
    "                value = temp\n",
    "                max_index.clear()\n",
    "                max_index.append(index)\n",
    "        # 행동의 확률 계산\n",
    "        prob = 1 / len(max_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 후에 [0.0,0.0,0.0,0.0]으로 초기화 했던 result에 max_index리스트에서 최대의 보상을 받게하는 행동의 인덱에 위에서 구한 확률을 넣는다. 그리고 다음 policy를 업데이트 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for index in max_index:\n",
    "            result[index] = prob\n",
    "            \n",
    "        next_policy[state[0]][state[1]] = result\n",
    "\n",
    "    self.policy_table = next_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책에 담긴 행동중 하나를 무작위로 실행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_policy, get_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "화면에 정책과 가치를 보여주기위한 함수들이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 다이내믹 프로그래밍2: 가치 이터레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 명시적인 정책과 내재적인 정책"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 정책 이터레이션에서\n",
    " - 명시적인(Explicit) 정책:\n",
    " <br>\n",
    " 정책과 가치함수가 명확히 분리되어 있을때\n",
    " - 내재적인(Implicit) 정책:\n",
    " <br>\n",
    " 정책이 가치함수 안에 내재적으로 포함되어 있을때\n",
    " <br>\n",
    " 가치함수를 업데이트하면 정책또한 자동적으로 발전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 벨만 최적 방정식과 가치 이터레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 기대 방정식을 통해 나오는 결과는 현재 정책을 시행했을때 받을 참 보상이다.\n",
    "1. 가치함수를 현재 정책에 대한 가치함수라고 가정\n",
    "2. 반복적 계산\n",
    "3. 현재 정책에 대한 참 가치함수\n",
    "<br>\n",
    "\n",
    "벨만 최적 방정식의 결과는 최적 가치함수이다. \n",
    "1. 가치함수를 최적 정책에 대한 가치함수라고 가정\n",
    "2. 반복적 계산\n",
    "3. 최적 정책에 대한 참 가치함수\n",
    "<br>\n",
    "\n",
    "그렇기 때문에 벨만 최적 방정식을 쓰는 가치 이터레이션은 정책 발전이 필요없다. \n",
    "<br><br>\n",
    "벨만 최적 방정식은 다음과 같다.\n",
    "$$v_*(s) = \\max_{a}E_\\pi[R_{t+1} + \\gamma v_*(S_{t+1})|S_t = s, A_t = a]$$\n",
    "<br>\n",
    "\n",
    "수식에 정책이 포함되어 있는 벨만 기대 방정식과 다르게 벨만 최적 방정식은 현재 상태에서 가능한 $R_{t+1} + \\gamma v_k(S_{t+1})$의 값들 중에서 최고의 값으로 업데이트하면 된다. 이것을 가치 이터레이션이라고 한다.\n",
    "<br><br>\n",
    "마찬가지로 계산 가능한 형태로 바꾸면,\n",
    "$$v_{k+1}(s) = \\max_{a \\in A}(R_s^a + \\gamma \\sum_{s' \\in S}P_{ss'}^a v_k(s'))$$\n",
    "이다. \n",
    "<br><br>\n",
    "그리드월드에서 상태 변환 확률을 1로 가정하기 때문에 식은 다음과 같이 된다.\n",
    "$$v_{k+1}(s) = \\max_{a \\in A}(R_s^a + \\gamma v_k(s'))$$\n",
    "<br>\n",
    "\n",
    "벨만 기대 방정식과 다르게 정책값을 이용해 기대값을 계산하는 부분이 없고 $\\max$가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 가치 이터레이션 코드 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from environment.env_value import GraphicDisplay, Env\n",
    "\n",
    "class ValueIteration:\n",
    "    def __init__(self, env):\n",
    "        # 환경 객체 생성\n",
    "        self.env = env\n",
    "        # 가치 함수를 2차원 리스트로 초기화\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # 감가율\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "    # 가치 이터레이션\n",
    "    # 벨만 최적 방정식을 통해 다음 가치 함수 계산\n",
    "    def value_iteration(self):\n",
    "        next_value_table = [[0.0] * self.env.width for _ in\n",
    "                            range(self.env.height)]\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = 0.0\n",
    "                continue\n",
    "            # 가치 함수를 위한 빈 리스트\n",
    "            value_list = []\n",
    "\n",
    "            # 가능한 모든 행동에 대해 계산\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value_list.append((reward + self.discount_factor * next_value))\n",
    "            # 최댓값을 다음 가치 함수로 대입\n",
    "            next_value_table[state[0]][state[1]] = round(max(value_list), 2)\n",
    "        self.value_table = next_value_table\n",
    "\n",
    "    # 현재 가치 함수로부터 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        action_list = []\n",
    "        max_value = -99999\n",
    "\n",
    "        if state == [2, 2]:\n",
    "            return []\n",
    "\n",
    "        # 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산\n",
    "        # 최대 큐 함수를 가진 행동(복수일 경우 여러 개)을 반환\n",
    "        for action in self.env.possible_actions:\n",
    "\n",
    "            next_state = self.env.state_after_action(state, action)\n",
    "            reward = self.env.get_reward(state, action)\n",
    "            next_value = self.get_value(next_state)\n",
    "            value = (reward + self.discount_factor * next_value)\n",
    "\n",
    "            if value > max_value:\n",
    "                action_list.clear()\n",
    "                action_list.append(action)\n",
    "                max_value = value\n",
    "            elif value == max_value:\n",
    "                action_list.append(action)\n",
    "\n",
    "        return action_list\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return round(self.value_table[state[0]][state[1]], 2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    value_iteration = ValueIteration(env)\n",
    "    grid_world = GraphicDisplay(value_iteration)\n",
    "    grid_world.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def value_iteration(self):\n",
    "        next_value_table = [[0.0] * self.env.width for _ in\n",
    "                            range(self.env.height)]\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = 0.0\n",
    "                continue\n",
    "            # 가치 함수를 위한 빈 리스트\n",
    "            value_list = []\n",
    "\n",
    "            # 가능한 모든 행동에 대해 계산\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value_list.append((reward + self.discount_factor * next_value))\n",
    "            # 최댓값을 다음 가치 함수로 대입\n",
    "            next_value_table[state[0]][state[1]] = round(max(value_list), 2)\n",
    "        self.value_table = next_value_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 최적 방정식에서는 $\\max$를 구해야 하기 때문에 $R_{s}^a + \\gamma v_k(s')$를 모든 행동에 대해 계산한뒤 value_list에 넣는다. 이후에 최고의 값을 새로운 가치함수로 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 현재 가치 함수로부터 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        action_list = []\n",
    "        max_value = -99999\n",
    "\n",
    "        if state == [2, 2]:\n",
    "            return []\n",
    "\n",
    "        # 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산\n",
    "        # 최대 큐 함수를 가진 행동(복수일 경우 여러 개)을 반환\n",
    "        for action in self.env.possible_actions:\n",
    "\n",
    "            next_state = self.env.state_after_action(state, action)\n",
    "            reward = self.env.get_reward(state, action)\n",
    "            next_value = self.get_value(next_state)\n",
    "            value = (reward + self.discount_factor * next_value)\n",
    "\n",
    "            if value > max_value:\n",
    "                action_list.clear()\n",
    "                action_list.append(action)\n",
    "                max_value = value\n",
    "            elif value == max_value:\n",
    "                action_list.append(action)\n",
    "\n",
    "        return action_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 최적 방정식을 통해 구한 가치함수를 토대로 Agent는 행동을 취한다. 최적 정책이 아니더라도 사용자는 현재 가치함수에 대한 탐욕 정책을 볼 수 있다. 탐욕 정책을 위해 각 행동별 큐함수를 구한뒤 가장큰 값은 action_list에 넣는다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 다이내믹 프로그래밍의 한계와 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 다이내믹 프로그래밍의 한계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다이내믹 프로그램은 계산 속도가 빠른 것이지 학습을 하는것이 아니다. 3가지 문제점이 존재한다.\n",
    "1. 계산 복잡도\n",
    "    - 다이내믹 프로그래밍의 계산 복잡도는 상태크기의 3제곱에 비례한다. 즉, 바둑같은 건 시간이 아주 오래 걸린다.\n",
    "2. 차원의 저주\n",
    "    - 상태의 차원이 늘어나면 상태의 수가 지수적으로 증가한다. 차원의 저주(Curse of Dimentionality)에 걸리는 것이다.\n",
    "3. 환경에 대한 완벽한 정보 필요\n",
    "    - 다이내믹 프로그래밍을 풀때 보상과 상태 변환 확률을 정확히 안다는 가정하에 풀었다. 이때 보상과 상태변환 확률을 '환경의 모델'이라고 하는데 이는 일반적인 경우 정확히 알 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 모델 없이 학습하는 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{환경의 모델} = P_{ss'}^a, R_s^a$$\n",
    "<br>\n",
    "\n",
    "모델(Model)이란 여기서는 수학적 모델으로 시스템에 입력이 들어왔을 때 시스템이 어떤 출력을 내는지에 대한 함수다. 이렇게 입력과 출력의 관계를 식으로 나타내는 과정을 모델링(Modeling)이라고 한다.\n",
    "<br><br>\n",
    "\n",
    "일반적인 경우, (특히, 자연의 경우) 정확하게 모델링 하는 것은 너무 어렵다. 이럴 경우에 입출력 사이의 관계를 알기 위해 2가지 방법으로 접근해볼 수 있다.\n",
    "1. 인간으로써 할 수있는 한 최고의 모델을 만든뒤 모델링 오차에 대한 부분을 실험을 통해 조정한다.\n",
    "2. 모델 없이 환경과 상호작용으로 입출력 사이의 관계를 학습한다.\n",
    "<br>\n",
    "\n",
    "1번이 고전적인 방법이고 시스템적으로 안정적이지만 문제가 복잡하고 어려울 수록 한계가 있다.\n",
    "<br>\n",
    "\n",
    "2번이 바로 강화학습의 방법이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
