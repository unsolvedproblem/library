{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fundamental of Reinforcement Learning(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 다이내믹 프로그래밍(Dynamic programming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다이내믹 프로그래밍은 작은 문제가 큰 문제 안에 중첩돼 있는 경우에 작은 문제의 답을 다른 작은 문제에 이용함으로써 효율적으로 계산하는 방법이다. 벨만(Bellman)이 만든 다이내믹 프로그래밍은 후에 강화학습의 근간이 된다. 다이내믹 프로그래밍의 한계를 극복하려고 나온게 강화학습이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 순차적 행동 결정 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화 학습은 순차적 행동 결정 문제를 푸는 방법 중 하나다. 앞서 배운 벨만 방정식을 이용해 순차적 행동 결정 문제를 푸는 방법을 정리하면 다음과 같다.\n",
    "1. 순차적 행동 문제를 MDP로 전환\n",
    "2. 가치함수를 벨만 방정식으로 반복적 계산\n",
    "3. 최적 가치함수와 최적 정책을 찾기\n",
    "<br><br>\n",
    "1번에 대해 했으니 이번에는 2,3번에 대해 알아보겠다.\n",
    "<br><br>\n",
    "\n",
    "벨만 방정식을 푼다는 것은\n",
    "$$v_*(s) = \\max_{a}E_\\pi[R_{t+1} + \\gamma v_*(S_{t+1})|S_t = s, A_t = a]$$\n",
    "을 만족하는 $v_*(s)$를 구하겠다는 뜻이다. 이는 큐함수에서도 마찬가지이다.\n",
    "<br><br>\n",
    "\n",
    "이 벨만 방정식을 계산하고 푸는 것이 바로 2, 3번에서 하고자 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 다이내믹 프로그래밍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순차적 행동 문제를 푸는 방법은 여러가지가 있다. 그중에 하나가 바로 강화학습이다. 그런데 강화학습 이전에 벨만 방정식을 푸는 알고리즘이 존재 했는데 그것이 바로 이 다이내믹 프로그래밍이다. 여기서 프로그래밍이란 '컴퓨터 프로그래밍'을 뜻하는 것이 아니라 계획을 하는 것으로 여러 프로세스가 다단계로 이뤄진다는 것을 의미한다.\n",
    "<br><br>\n",
    "\n",
    "다이내믹 프로그래밍의 기본 아이디어는 작은 문제들이 중첩된 큰 문제를 그 작은 문제들로 쪼개서 풀겠다는 것이다. 여기서 작은 문제들이 하나의 프로세스가 되는 것이고 이것을 다단계로 풀어나가는 것이 프로그래밍이 된다. 하나의 프로세스를 대상으로 푸는 것이 아니라 시간에 따라 다른 프로세스들을 풀어나가기 때문에 다이내믹 프로그래밍이라 부른다.\n",
    "<br><br>\n",
    "\n",
    "각각의 작은 문제들은 서로 연관되어있기 때문에 해답을 서로 이용할 수 있다. 이 특성을 사용해 계산량을 줄인다. \n",
    "<br><br>\n",
    "\n",
    "예를 들어 세 개의 상태가 있다고 가정하겠다. 문제의 목표는 각 상태의 참 가치함수($v_\\pi(s_1)$,$v_\\pi(s_2)$,$v_\\pi(s_3)$)를 구하는 것이고 이것이 하나의 큰 문제이다. 이때 작은 문제로 쪼개서 풀겠다는 것은 저 문제를 한번에 푸는 것이 아니라 \n",
    "$$v_0(s)\\to v_1(s)\\to \\cdots \\to v_k(s)\\to \\cdots \\to v_\\pi(s)$$\n",
    "<br>\n",
    "위와 같이 반복적인 계산을 통해 풀겠다는 것이다. 이 계산은 모든 상태에 대해 한 번 계산이 끝나면 모든 상태의 가치함수를 업데이트 한다. 다음 계산은 업데이트된 가치함수를 이용해 같은 과정을 반복한다. 이런 방식으로 효율적인 계산이 이루어 진다.\n",
    "<br><br>\n",
    "\n",
    "다이내믹 프로그래밍에는 정책 이터레이션(Policy iteration)과 가치 이터레이션(Value iteration)이 있다. 정책 이터레이션은 벨만 기대 방정식을, 가치 이터레이션은 벨만 최적 방정식을 푸는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 격자로 이뤄진 간단한 예제: 그리드 월드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP에서 했던 그리드 월드 예제를 그대로 다이나믹 프로그래밍의 예시로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 다이내믹 프로그래밍1: 정책 이터레이션"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 강화학습 알고리즘의 흐름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL알고리즘 흐름도](.\\image\\RL_algorithm_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP로 정의되는 문제의 목표는 Agent가 받는 총 보상을 최대로 하는 것이다. Agent는 가치 함수를 통해 이 목표에 얼마나 다가갔는지 알 수 있다. 이 가치함수에 대한 방정식이 벨만 방정식이다. \n",
    "<br><br>\n",
    "\n",
    "정책 이터레이션과 가치 이터레이션은 살사(SARSA)로 발전하고, 살사는 오프폴리시(Off-policy) 방법으로 변형되 큐러닝(Q-learning)으로 이어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 정책 이터레이션(Policy iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 이 문제에서 알고 싶은 것은 가장 높은 보상을 얻게하는 정책이다. 처음에는 이 정책을 알 수 없으니 우리는 어떤 특정한 정책으로 부터 발전시켜나가야 한다. 보통 무작위로 행동하는 정책(Random policy)으로 시작한다. 정책 이터레이션에서는 정책을 평가(Policy evaluation)하고 정책을 발전(Policy improvement)한다. 이 과정을 반복하면 정책은 최적의 형태로 수렴한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 정책 평가(Policy evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 평가는 가치 함수로 한다. 그래서 사용하는 것이 바로 벨만 기대 방정식이다.\n",
    "$$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$$\n",
    "<br>\n",
    "이를 계산 가능한 형태로 바꾸면\n",
    "$$v_\\pi(s)=\\sum_{a \\in A} \\pi (a|s)(R_{t+1} + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_\\pi (s'))$$\n",
    "이 된다. 하지만 그리드 월드에서는 상태 변환 확률을 1이라고 가정하기 때문에 식은 다음과 같이 된다.\n",
    "$$v_\\pi(s)=\\sum_{a \\in A} \\pi (a|s)(R_{t+1} + \\gamma v_\\pi (s'))$$\n",
    "<br>\n",
    "정책 평가는 $\\pi$라는 정책에 대해 반복적으로 수행하는 것이다. 따라서 계산 단계를 $k$ $(k=1,2,3,\\dots)$라고 할 때 $k$번째 가치함수를 통해 $k+1$번째 가치함수를 계산하는 방정식은 다음과 같다.\n",
    "$$v_{k+1}(s)=\\sum_{a \\in A} \\pi (a|s)(R_{t+1} + \\gamma v_k (s'))$$\n",
    "<br><br>\n",
    "\n",
    "한 번의 정책 평가 과정을 순서대로 나타내면 다음과 같다.\n",
    "1. $k$번째 가치함수 행렬에서 현재 상태 $s$에서 갈 수 있는 다음 상태 $s'$에 저장되어 있는 가치 함수 $v_k(s')$을 불러온다.\n",
    "2. $v_k(s')$에 감가율 $\\gamma$를 곱하고 그 상태로 가능 행동에 대한 보상 $R_s^a$를 더한다.\n",
    "$$R_s^a + v_k(s')$$\n",
    "3. 2번에서 구한 값에 정책 값을 곱한다.\n",
    "$$\\pi(a|s)(R_s^a + \\gamma v_k(s'))$$\n",
    "4. 3번을 모든 선택가능한 행동에 대해 반복하고 합친다.\n",
    "$$\\sum_{a \\in A} \\pi (a|s)(R_{t+1} + \\gamma v_k (s'))$$\n",
    "5. 4번의 값을 $k+1$번째 가치 함수 행렬의 상태 $s$자리에 저장한다.\n",
    "6. 1-5의 과정을 모든 $s \\in S$에 대해 반복한다.\n",
    "<br><br>\n",
    "\n",
    "이를 무한번 반복하면 참 $v_\\pi$를 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 정책 발전(Policy improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책을 평가를 했으면 발전을 시켜야 된다. 여러 방법 중에 탐욕 정책 발전(Greedy policy improvement)이라는 방법이 있다. 아이디어는 다음 상태 중에 가장 가치 함수가 높은 곳으로 가겠다는 것이다.\n",
    "<br><br>\n",
    "\n",
    "정책 평가를 통해 Agent가 정책을 따랏을 때의 모든 상태에 대한 가치를 구했다면, 이제 큐함수를 통해 어떤 행동이 좋은지 알아낼 수 있다. 계산 가능한 형태의 큐함수는 다음과 같다.\n",
    "\n",
    "$$q_\\pi(s,a) = R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_\\pi (s')$$\n",
    "<br>\n",
    "여기서 Agent가 할 일은 선택 가능한 행동의 $q_\\pi(s,a)$중에서 max값을 가지는 행동을 취하면 된다. 이것이 탐욕 정책 발전이다. 수식으로 표현하면 다음과 같다.\n",
    "$$\\pi'(s) = \\def\\argmax{\\text{argmax}} \\argmax_{a\\in A} q_\\pi(s,a)$$\n",
    "<br><br>\n",
    "\n",
    "이제 이 새롭게 업데이트된 정책으로 다시 정책 평가가 들어가고 정책 발전이 이루어진다. 이를 반복하므로써 기존의 정책은 최적의 정책으로 수렴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
