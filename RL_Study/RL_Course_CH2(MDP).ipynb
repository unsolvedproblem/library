{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fundamental of Reinforcement Learning(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Markov decision process(MDP)\n",
    "- 벨만 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습은 순차적으로 행동을 계속 결정하는 문제다. 이것을 수학적으로 표현한 것이 MDP이다.\n",
    "<br><br>\n",
    "\n",
    "MDP의 구성요소\n",
    "- 상태(State)\n",
    "- 행동(Action)\n",
    "- 보상 함수(Reward function)\n",
    "- 상태 변환 확률(State transition probablitiy)\n",
    "- 감가율(Discount factor)\n",
    "<br>\n",
    "\n",
    "문제를 풀기위해서 사람은 그 문제가 어떤 문제인지 그 출제자의 의도에 대해 파악하려 한다. 그리고 우리는 그 의도에 맞게 문제를 풀며 학습을 한다. 하지만 Agent는 그렇게 똑똑하지 않다. 출제자의 의도에 맞게 문제를 잘 정의해서 Agent에게 알려주어야 한다. '파이선과 케라스로 배우는 강화학습' 책에서 이렇게 말한다.\n",
    ">'에이전트를 구현하는 사람은 학습하기에 많지도 않고 적지도 않은 적절한 정보를 에이전트가 알 수 있도록 문제를 정의해야 합니다.'\n",
    "\n",
    "MDP의 이해를 위해 그리드 월드(Grid world)라는 예제를 살펴 보겠다.\n",
    "![image.png](https://image.slidesharecdn.com/rl-180829045645/95/rl-16-638.jpg?cb=1535518656)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 상태(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S$는 Agent가 관찰 가능한 상태의 집합이다. 상태라는 말은 \"자신의 상황에 대한 관찰\"으로 표현할 수 있다. 게임을 학습하기 위한 에이전트는 사용자가 상태를 정의해 주어야 한다. 이때 에이전트가 학습하기에 충분할 정도의 정보가 상태에 담겨있어야 한다.\n",
    "<br><br>\n",
    "위 그리드 월드의 상태를 집합으로 나타내면,\n",
    "$$S = \\{(1,1),(1,2),(1,3),\\dots, (5,5)\\}$$\n",
    "가 된다.\n",
    "<br><br>\n",
    "Agent가 (1,1)에 있으면 Agent의 상태는 (1,1)이 된다.\n",
    "<br><br>\n",
    "Agent는 시간에 따라 S안에 있는 25개의 상태를 탐험하게 된다. 시간을 $t$라고 했을때 상태를 $S_t$라고 표현하는데 만얀 시간이 t일 때 상태가 (1,3)이라면 다음과 같이 표현한다.\n",
    "$$S_t = (1,3)$$\n",
    "특정 $t$에서의 상태 $S_t$는 정해진것이 아니다. 뽑을때 마다 달라질수 있다. 그래서 이를 확률변수(Random variable)이라고 한다.\n",
    "<br><br>\n",
    "'시간 $t$에서의 상태 $S_t$가 어떤 상태 $s$다' 라는 것은 다음과 같이 표현한다.\n",
    "$$S_t = s$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 행동(Action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent가 상태 $S_t$에서 할 수 있는 행동의 집합을 $A$라고 표현한다. 일반적으로 Agent가 할 수 있는 행동은 모든 상태에서 같기 때문에 하나의 집합 A로 표현 가능하다. 특정한 행동을 보통 $a$로 표현을 하고, '시간 t에 Agent가 특정한 행동 a를 했다'는 다음과 같이 표현한다.\n",
    "$$A_t = a$$\n",
    "($A_t$는 어떤 $t$라는 시간에 집합 $A$에서 선택한 행동이라는 뜻)\n",
    "<br><br>\n",
    "그리드 월드에서 Agent가 할 수 있는 행동의 집합은\n",
    "$$A=\\{up, down, left, right\\}$$\n",
    "이다.\n",
    "<br><br>\n",
    "시간 $t$에서 상태가 $(3,1)$이고 $A_t = right$라면 $t+1$의 상태는 $(4,1)$이 된다.\n",
    "<br><br>\n",
    "만약 바람과 같은 예상치 못한 요소가 있다면 Agent는 $(4,1)$로 가지 못했을 것이다. 이런 상황을 고려해주는 것이 상태 변환 확률이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 보상함수(Reward function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보상은 Agent가 학습할 수 있는 유일한 정보로, 환경이 Agent에게 주는 정보이다. 시간 $t$에서 상태가 $S_t=s$이고 행동이 $A_t=a$일 때 에이전트가 받을 보상은 다음과 같다.\n",
    "$$R_s^a = E[R_t|S_t=s, A_t=a]$$\n",
    "($E$는 기댓값(Expectation))\n",
    "<br><br>\n",
    "기댓값이 무엇인지 이해하기 위해 주사위를 예로 들겠다. 1에서 6을 가진 주사위가 있고 이 주사위의 모든 면이 동등한 확률로 나온다고 했을 때 기대값은 다음과 같다.\n",
    "$$\\text{기댓값} = 1 \\times \\frac{1}{6} + 2 \\times \\frac{1}{6} + 3 \\times \\frac{1}{6} + 4 \\times \\frac{1}{6} + 5 \\times \\frac{1}{6} + 6 \\times \\frac{1}{6}=\\frac{21}{6}$$\n",
    "<br>\n",
    "즉, 기댓값은 나오게 될 숫자에 대한 예상이다. 보상 또한 기댓값과 같이 어떤 상태 $s$에서 행동 $a$를 했을 때 받을 것이라 예상되는 숫자다. 그렇다면 왜 보상함수르 기댓값으로 표현할까? 그것은 환경에 따라 같은 상태에서 같은 행동을 해도 다른 보상을 받을 수 있기 때문이다. 그 모든 것을 고려해 보상함수를 기댓값으로 표현한다.\n",
    "<br><br>\n",
    "그리고 주의 깊게 봐야할 것은 보상은 $t+1$에서 받는다는 것이다. $t$에서의 상태$s$에서 행동$a$를 했기 때문에 Agent는 $t+1$에서 보상을 받는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 상태 변환 확률(State transition probalaility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent가 상태 $s$에서 행동 $a$를 해서 $s'$로 가게될 확률이 바로 상태 변환 확률이다.\n",
    "$$P_{ss'}^a = P[S_{t+1} = s' | S_t =s, A_t = a]$$\n",
    "이 값은 Agent가 알지못하는 값으로 환경의 일부다. 상태 변환 확률은 환경의 모델(Model)이라고도 부른다. 환경은 Agent가 행동을 취하면 상태 변환 확률을 통해 다음 Agent가 갈 상태를 알려준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 감가율(Discount factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 보상이면 나중에 받을 수록 가치가 줄어든다는 것이다. 수식으로 $\\gamma$라고 표기한다. 감가율은 0에서 1사이의 값이다.\n",
    "<br><br>\n",
    "현재의 시간 $t$로부터 시간 $k$가 지난 후에 보상을 $R_{t+k}$를 받는다고 하면 식은\n",
    "$$\\gamma^{k-1}R_{t+k}$$\n",
    "가 된다. 현재로부터 $k$만큼 시간이 지났기 때문에 미래에 받을 보상 $R_{t+k}$는 $\\gamma^{k-1}$만큼 감가된다.\n",
    "\n",
    "왜 감가율을 적용시킬까? 바로 최적의 루트를 찾기 위해서 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 정책(Policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책은 모든 상태에서 Agent가 할 행동이다. 상태가 입력으로 들어오면 행동을 출력으로 내보내는 일종의 함수라고 봐도 된다. 정책은 각 상태에서 하나의 행동만 나타낼 수도 있고 확률적으로 $a_1=10\\%, a_2=90\\%$로 나타낼 수도 있다.\n",
    "<br><br>\n",
    "최적의 정책은 각 상태에서 하나의 행동만 선택하지만 Agent가 학습중일 때는 확률적으로 여러 행동을 선택할 수 있어야한다. 수식으로 나타내면 다음과 같다.\n",
    "$$\\pi(a|s) = P[A_t=a|S_t=s]$$\n",
    "시간 $t$에 $S_t=s$에 Agent가 있을 때 가능한 행동 중에서 $A_t=a$를 할 확률을 나타낸다.\n",
    "<br><br>\n",
    "정책을 가지고 있으면 Agent는 모든 상태에서 자신이 해야 할 행동을 알 수 있다. 강화학습 문제를 통해 알고 싶은 것은 '최적의 정책'이다. 강화 학습은 이 최적의 정책을 얻기 위해 학습해나가는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Agent and Env](https://image.slidesharecdn.com/random-170812084307/95/-23-638.jpg?cb=1502527620)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 MDP를 통해 순차적 행동 결정 문제를 정의했다. Agent가 현재 상태에서 앞으로 받을 보상을 고려해 행동을 결정하고, 환경은 Agent에게 실제 보상과 다음 상태를 알려준다.\n",
    "<br><br>\n",
    "이런 과정을 반복하면서 Agent는 앞으로 받을 것이라 예상했던 보상과 실제 받는 보상을 비교하면서 자신의 정보와 정책을 바꿔나간다. 이 학습 과정을 되풀이 하면 할수록 더 가장 최고의 보상을 받는 정책을 학습할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 가치함수(Value fucntion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 가치함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떠한 상태에 이으면 앞으로 얼마의 보상을 받을 것인지에 대한 기대값이 바로 가치함수이다. Agent는 가치함수를 통해 행동을 선택한다. t수식으로 나타내기에 앞서 반환값(Return) $G_t$을 먼저 정의한다. $G_t$는 타임 스텝 $t$부터 시작하는 감가된 보상들의 총 합(Total discounted reward from $t$)이다.\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^∞ \\gamma^k R_{t+k+1}$$\n",
    "<br><br>\n",
    "여기서 감가를 하는 이유는\n",
    "- 수학적인 편의를 위해\n",
    "- cyclic Markov processes에서 무한대로 발산하는 것을 피하기 위해\n",
    "- 미래의 불확실성이 모두 표현되지 않을 수 있어서\n",
    "- 현재의 보상과 미래의 보상을 구분할 수 없어서\n",
    "- 사람이나 동물의 행동은 즉각적인 보상을 더 선호해서\n",
    "<br>\n",
    "\n",
    "(때때로 감가가 안된 Markov reward processes (i.e. $\\gamma = 1$)을 쓰기도 한다. 예를들면 모든 시퀀스가 끝이있을때.)\n",
    "<br><br>\n",
    "각 타임 스텝마다 받는 보상은 확률 변수이지만 가치함수는 특정 양을 나타내는 값이다. 가치 함수를 수식으로 나타내면 다음과 같다.\n",
    "$$v(s) = E[G_t|S_t =s]$$\n",
    "<br><br>\n",
    "여기에 반환값 수식을 대입하면,\n",
    "$$v(s) = E[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots|S_t =s]$$\n",
    "$$v(s) = E[R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\dots)|S_t =s]$$\n",
    "$$v(s) = E[R_{t+1} + \\gamma(G_{t+1})|S_t =s]$$\n",
    "$$v(s) = E[R_{t+1} + \\gamma v(S_{t+1})|S_t =s]$$\n",
    "<br>\n",
    "가치 함수를 두 부분으로 나눌 수 있게 됩니다.\n",
    " - 즉각적인 보상 $R_{t+1}$\n",
    " - 후에오는 상태의 감가된 값 $\\gamma v(S_{t+1})$\n",
    "<br><br>\n",
    "\n",
    "위의 가치 함수에서 정책을 고려하면 식은 다음과 같이 표현이 된다.\n",
    "$$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$$\n",
    "<br>\n",
    "이것을 벨만 기대 방정식(Bellman Expectation Equation)이라고 부른다. 이것은 현제 상태의 가치 함수$v_\\pi(s)$와 다음 상태의 가치 함수$v_\\pi(S_{t+1})$ 사이의 관계를 말해주는 방정식이다. 강화학습은 벨만 방정식을 어떻게 풀어가느냐 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 큐함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가치 함수는 상태가 입력으로 들어오면 그 상태에서 앞으로 받을 보상의 합을 출력하는 함수이다. Agent는 이 가치 함수를 통해 어느 상태에 있는 것이 좋을지, 어느 상태로 가야할지 판단할 수 있다.\n",
    "<br><br>\n",
    "만약 어떤 상태에서 각 행동에 대해 따로 가치함수를 만들어서 그 정보를 얻어올 수 있다면 굳이 다음 상태들의 가치함수를 따져보지 않아도 어떤 행동을 해야 할지 결정할 수있다. 이처럼 어떤 상태에서 어떤 행동이 얼마나 좋은지 알려주는 함수를 행동 가치함수라고 하고 간단하게 큐함수(Q function)이라고 부른다.\n",
    "\n",
    "![큐함수](https://image.slidesharecdn.com/random-170812084307/95/-35-638.jpg?cb=1502527620)\n",
    "<br><br>\n",
    "그림과 같이 하나의 상태에서 2개의 행동이 있을때 이 행동들에서 각각 따로 가치 함수를 계산할 수 있는데 그것이 바로 큐함수이다. 따라서 큐함수는 변수로 상태, 행동을 받고 수식으로 표현하면 $q_\\pi(s,a)$가 된다.\n",
    "<br><br>\n",
    "\n",
    "가치함수와 큐함수 사이의 관계는 다음과 같이 표현된다.\n",
    "$$v_\\pi(s)=\\sum_{a \\in A} \\pi (a|s) q_\\pi (s,a)$$\n",
    "<br>\n",
    "\n",
    "강화학습에서 Agent가 행동을 선택하는 기준으로 가치함수보다는 큐함수를 사용한다. 큐함수 또한 벨만 기대 방정식의 형태로 나타낼 수 있다. 가치함수와 다른점은 조건에 행동이 들어간다는 것이다.\n",
    "$$q_\\pi(s,a) = E_\\pi[R_{t+1} + \\gamma q_\\pi(S_{t+1},A_{t+1})|S_t = s, A_t = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 벨만 방정식 (Bellman equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 벨만 기대 방정식(Bellman expectation equatio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$$\n",
    " <br>\n",
    "이 식이 바로 벨만 기대 방정식이다. 이 식을 계산 가능한 형태의 벨만 기대 방정식으로 나타내면 다음과 같다.\n",
    "$$v_\\pi(s)=\\sum_{a \\in A} \\pi (a|s)(R_{t+1} + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_\\pi (s'))$$\n",
    "<br>\n",
    "이 식의 유도 과정은 가치함수와 큐함수사이의 관계식으로 부터 시작한다.\n",
    "$$v_\\pi(s)=\\sum_{a \\in A} \\pi (a|s) q_\\pi (s,a)$$\n",
    "<br>\n",
    "큐함수는\n",
    "$$q_\\pi(s,a) = R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_\\pi (s')$$\n",
    "<br>\n",
    "이고, 이를 $v_\\pi$에 관한 식으로 바꾸면,\n",
    "$$v_\\pi(s)=\\sum_{a \\in A} \\pi (a|s) (R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_\\pi (s'))$$\n",
    "<br>\n",
    "이된다. 그리고 $q_\\pi$에 관한 식으로 바꾸면,\n",
    "$$q_\\pi(s,a) = R_s^a + \\gamma \\sum_{s' \\in S} P_{ss'}^a \\sum_{a' \\in A} \\pi (a'|s') q_\\pi (s',a')$$\n",
    "<br>\n",
    "이 된다.\n",
    "<br>\n",
    "예제를 통해 알아보자.\n",
    "<br><br>\n",
    "3x3의 그리드 월드에서 현재 Agent가 가운데 있다. 행동은 '상,하,좌,우'로 할 수 있으며 각 행동은 25%의 확률로 선택된다.(이때 상태 변환 확률은 모든 s와 a에 대해 1이라고 가정한다.) Agent의 상태에 저장된 가치함수는 0, 왼쪽 상태의 가치함수는 1, 밑의 상태의 가치함수는 0.5, 위의 상태의 가치 함수는 0, 오른쪽 상태의 가치함수는 0이다. 그리고 오른쪽으로 행동을 취하면 1의 보상을 받는다. 감가율은 0.9로 정한다.\n",
    "<br><br>\n",
    "지금 상태 변환 확률을 항상 1로 가정했기 때문에 수식은 다음과 같이 변한다.\n",
    "$$v_\\pi(s)=\\sum_{a \\in A} \\pi (a|s)(R_{t+1} + \\gamma v_\\pi (s'))$$\n",
    "<br>\n",
    "(1)각 행동에 대해 그 행동을 할 확률을 고려하고 (2)각 행동을 했을 때 받을 보상과 (3)다음 상태의 가치함수를 고려해서 계산하면\n",
    "1. 행동 = 상 $0.25 \\times (0 + 0.9 \\times 0 ) = 0$\n",
    "2. 행동 = 하 $0.25 \\times (0 + 0.9 \\times 0.5 ) = 0.1125$\n",
    "3. 행동 = 좌 $0.25 \\times (0 + 0.9 \\times 1 ) = 0.225$\n",
    "4. 행동 = 우 $0.25 \\times (1 + 0.9 \\times 0 ) = 0.25$\n",
    "- 총합 : 기댓값 = $0+0.1125+0.225+0.25=0.5875$\n",
    "<br><br>\n",
    "\n",
    "이렇게 벨만 기대 방정식을 이용해 현재의 가치함수를 계속 업데이트하다 보면 참값을 구할 수 있다. 여기서 참값이란 최대의 보상을 뜻하는 것이 아니라 현재의 정책을 따랐을 경우 에이전트가 얻을 실제 보상에 대한 참 기대값이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 벨만 최적 방정식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$$\n",
    " 이 벨만 기대 방정식을 계속 진행하면(무한히 진행하면) 언젠가 식의 왼쪽 항과 오른쪽 항이 동일해 진다. 즉 수렴한다는 것이다. 그렇다면 현재 정책 $\\pi$에 대한 참 가치함수를 구한것이다.\n",
    "<br><br>\n",
    "\n",
    "참 가치함수와 최적 가치함수(Optimal value function)은 다른 것이다. 참 가치함수는 \"어떤 정책\"을 따라 움직였을 때 얻게되는 보상의 참값이다. 최적 가치함수는 수많은 정책 중에서 가장 높은 보상을 주는 가치함수이다.\n",
    "<br><br>\n",
    "\n",
    "참 가치함수는 벨만 기대 방정식을 매트릭스로 해서 풀면 풀 수 있다. 하지만 계산 복잡도가 $O(n^3)$이 되기 때문에 작은 MDP에만 적용할 수있다. 큰 MDP에 적용할수 있는 여러 알고리즘이 존재한다.(예를들면 다이내믹 프로그래밍)\n",
    "<br><br>\n",
    "\n",
    "가치 함수가 바로 정책이 좋은지 나쁜지 판별해주는 녀석이다. 따라서 모든 정책에 있어서 가장 큰 가치함수 결과를 뱉어내는 것이 최적의 정책이다. 최적의 가치 함수는 다음과 같이 표현된다.\n",
    "$$v_*(s) = \\max_{\\pi}[v_\\pi(s)]$$\n",
    "<br>\n",
    "최적의 큐함수는 다음과 같이 표현된다.\n",
    "$$q_*(s,a) = \\max_{\\pi}[q_\\pi(s,a)]$$\n",
    "<br>\n",
    "\n",
    "가장 높은 가치함수(큐함수)를 에이전트가 찾았다고 할때 최적 정책은 그 가장 높은 큐함수를 가진 행동을 하는 것이다. 수식으로 표현하면 다음과 같다.\n",
    "$$\n",
    "\\pi_*(s,a) =\n",
    "\\begin{cases}\n",
    "1,  & \\text{if $a = argmax_{a \\in A} q_*(s,a)$} \\\\\n",
    "0,  & otherwise\n",
    "\\end{cases} $$\n",
    "\n",
    "최적의 가치함수 혹은 큐함수를 구하는 것이 순차적 행동 결정 문제를 푸는 것이다. 최적의 큐함수 중에서 max를 취하는 것이 최적의 가치함수가 된다.\n",
    "$$v_*(s) = \\max_{a}[q_*(s,a)|S_t = s, A_t = a]$$\n",
    "<br>\n",
    "그리고 여기서 큐함수를 가치함수로 고치면 다음과 같이 된다.\n",
    "$$v_*(s) = \\max_{a}E_\\pi[R_{t+1} + \\gamma v_*(S_{t+1})|S_t = s, A_t = a]$$\n",
    "<br>\n",
    "이를 벨만 최적 방정식(Bellman optimality equation)이라 한다. 큐함수에 대해서도 벨만 최적 방정식을 표현할 수 있다. \n",
    "$$q_*(s,a) = E[R_{t+1} + \\gamma \\max_{a'} q_*(S_{t+1},a')|S_t = s, A_t = a]$$\n",
    "<br>\n",
    "최적 정책을 따라갈 때 현재 상태의 큐함수는 다음 상태에 선택 가능한 행동중 가장 높은 값의 큐함수에 1번 감가하고 보상을 더한것과 같다.\n",
    "<br><br>\n",
    "\n",
    "위 두 수식들에 $E$가 들어가는 이유는 이 함수들 자체가 행동까지 선택한 상황이고 보상은 환경에 따라 달라질 수도 있는 값이기 때문이다. 이 두 방정식을 이용해 MDP로 정의되는 문제를 계산으로 푸는것이 바로 다이내믹 프로그래밍이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
