{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 About Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습을 알아보기에 앞서 강화학습을 포함하는 더 큰 범주인 Machine learning의 범주에 대해 간단하게 알아보겠다.\n",
    "\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "3. Reinforcement Learning\n",
    "<br>\n",
    "\n",
    "\n",
    "- Supervised Learning : 지도 학습으로 정답이 있어서 바로바로 피드백을 받으며 학습하는 것\n",
    "- Unsupervised Learning : 비지도 학습으로 정답 없이 학습하여 분류같은 문제를 해결하는 방법\n",
    "- Reinforcement Learning : 정답은 아니지만 행동에 대한 보상이 주어져 그것을 토대로 학습하는 것\n",
    "<br>\n",
    "\n",
    "\n",
    "위키 피디아에서는 강화학습을 다음과 같이 설명한다.\n",
    ">강화 학습(Reinforcement learning)은 기계학습이 다루는 문제 중에서 다음과 같이 기술 되는 것을 다룬다. 어떤 환경을 탐색하는 에이전트가 현재의 상태를 인식하여 어떤 행동을 취한다. 그러면 그 에이전트는 환경으로부터 포상을 얻게 된다. 포상은 양수와 음수 둘 다 가능하다. 강화 학습의 알고리즘은 그 에이전트가 앞으로 누적될 포상을 최대화하는 일련의 행동으로 정의되는 정책을 찾는 방법이다.\n",
    "<br>\n",
    "\n",
    "\n",
    "예를 들면, 한국 대학생 술자리 문화중에 술게임이란것이 있다. 처음 술게임을 배울때 흔히 '마시면서 배운다' 라는 표현이 있는데 이것이 바로 강화학습의 한 예이다. 처음 대학에 들어간 신입생은 새로운 게임을 접하고 일딴 다른사람의 행동을 무작위로 취해본다. 이때 맞으면 넘어가고 틀리면 술을 마시게 된다. 여기서 술을 마시는것이 (-)포상이고 마시지 않는것이 (+)포상인 것이다. 그렇게 몇번 하고나면 그 신입생은 그 게임을 잘하게 된다. 이것이 강화학습의 일련의 과정이다.\n",
    "<br>\n",
    "\n",
    "\n",
    "강화학습도 이와 마찬가지로 Agent가 아무정보도 없이 Environment에 들어가 특정 행동을 취한다. 그리고 포상의 정도로 다음 행동을 결정한다. 이러한 경험들을 통해 학습하는것이 바로 강화학습이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Characteristic of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습은 다음과 같은 특성들을 가진다.\n",
    "1. No supervisor, only a reward signal\n",
    "2. Feedback is delayed, not instantaneous\n",
    "3. Time really matters\n",
    "4. Agent's actions affect the subsequent data it receives\n",
    "<br>\n",
    "\n",
    "\n",
    "- 지도자가 없다.(즉, 정답없이 학습된다). 오직 포상만 있을 뿐이다. Trial and Error, 스스로 시도를 거듭하면서 자신을 조정한다는 것이다.\n",
    "- 피드백이 지연된다. 어떠한 선택을 했을때 그 선택에 대한 궁극적 포상이 바로 나오지 않는다는 것이다. 선택한 행동이 당장 (-)포상일지라도 몇 스탭 후에는 그것이 더 큰 (+)포상을 가져다 줄 수 있다. 강화학습의 목표는 최종적으로 얻는 포상의 최대화이기 때문에 즉각적인 피드백은 나올 수 없다.\n",
    "- 시간이라는 개념이 포함이 된다. 위의 내용과 연관되어 포상을 위한 일련의 행동을 연속적으로 결정했기 때문이다. \n",
    "- 마지막으로 Agent의 각 행동은 그 다음의 데이터에 영향을 준다. 즉, Agent의 행동이 Enviornment에 영향을 준다는 것이다.\n",
    "<br>\n",
    "\n",
    "\n",
    "강화학습의 예들을 살펴보겠다,\n",
    "1. Fly stunt manoeuvres in a helicopter\n",
    "2. Defeat the world champion at Backgammon\n",
    "3. Manage an investment portfolio\n",
    "4. Control a power station\n",
    "5. Make a humanoid robot walk\n",
    "6. Play many different Atari games better than humans\n",
    "<br>\n",
    "\n",
    "\n",
    "다양한 예제들에서 알 수 있듯이 강화학습은 정말 다양한 필드에 적용시킬 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Elements of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습을 이루고 있는 요소들에 대해 알아보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward $R_t$는 Scalar feedback signal이다. 그 스탭(행동)이 얼마나 잘 했나 하는 것을 평하하는 지표이다. Agent의 일은 이것을 최대화하는 것이다. 앞의 예들에서 reward로 무엇을 택했는지 살펴보자.\n",
    "<br>\n",
    "\n",
    "\n",
    "- Fly stunt manoeuvres in a helicopter\n",
    "    - +ve reward for following desired trajectory\n",
    "    - -ve reward for crashing\n",
    "- Defeat the world champion at Backgammon\n",
    "    - +/-ve reward for winning/losing a game\n",
    "- Manage an investment portfolio\n",
    "    - +ve reward for each $ in bank\n",
    "- Control a power station\n",
    "    - +ve reward for producing power\n",
    "    - -ve reward for exceeding safety thresholds\n",
    "- Make a humanoid robot walk\n",
    "    - +ve reward for forward motion\n",
    "    - -ve reward for falling over\n",
    "- Play many different Atari games better than humans\n",
    "    - +/-ve reward for increasing/decreasing score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Agent and Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AnE](.\\image\\agent_and_environment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 스텝마다 Agent는\n",
    " - action $A_t$을 하고\n",
    " - observation $O_t$를 받고\n",
    " - scalar reward $R_t$를 받음\n",
    "<br>\n",
    "\n",
    "\n",
    "각 스텝마다 Enviroment는\n",
    " - action $A_t$를 받고\n",
    " - observation $O_{t+1}$를 주고\n",
    " - scalar reward $R_{t+1}$를 줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 History and State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History\n",
    "History는 Observation, Action, Rewards의 연속이다.\n",
    "$$H_t = O_1,R_1,A_1, \\dots, A_{t-1}, O_t,R_t$$\n",
    "<br>\n",
    "History를 바탕으로 Agent는 action을 선택하고, Environment는 observation과 rewards를 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State\n",
    "State는 다음에 뭐가 올지 결정하는데 사용되는 정보이다. 일반적으로 아래와 같이 표현된다.\n",
    "$$S_t = f(H_t)$$\n",
    "\n",
    "State에는 3가지가 있다. \n",
    "1. Environment state\n",
    "2. Agent state\n",
    "3. Information state(a.k.a Markov state)\n",
    "<br>\n",
    "\n",
    "\n",
    "- Environment state $S_t^e$: 환경의 private representation이다. 다음 observation과 reward를 뽑는데 쓰이는 데이터이다.\n",
    "<br><br>\n",
    "- Agent state $S_t^a$: Agent's internal representation이다. 다음 action을 선택하는데 사용된다. 즉 강화학습 알고리즘에 사용되는 정보이다. 이것은 아래와 같이 표현할 수 있다.\n",
    "$$S_t^a = f(H_t)$$\n",
    "<br><br>\n",
    "- Information state(a.k.a Markov state) : 이것은 History의 모든 유용한 정보를 담고있다.\n",
    "<br>\n",
    "어떤 state $S_t$가 Markov 한다는 것은\n",
    "$$\\mathbb{P}[S_{t+1}|S_t] = \\mathbb{P}[S_{t+1}|S_1,\\dots,S_t]$$\n",
    "과 동치다.\n",
    "<br>\n",
    "그대로 해석을 하자면, 미래는 현재를 있게 해준 과거와 독립관계라는 뜻이다. 즉, 현재 State가 과거의 정보들을 다 내포하고 있으니 현재의 State로만 미래를 예측해도 상관이 없다는 것이다.<br><br>\n",
    "따라서 Information state는 Markov state로 불리기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment에도 두가지 종류가 있다.\n",
    "1. Fully Observable Environment\n",
    "2. Patially Observable Environment\n",
    "<br>\n",
    "\n",
    "\n",
    "- Fully Observable Environments: agent가 직접 Environment state를 관찰한다. 즉,\n",
    "$$ O_t = S_t^a = S_t^e$$<br>\n",
    "Agent state = Environment state = Information state 인 상황이다.<br>\n",
    "일반적으로 이것을 Markov decision process(MDP)라고 한다.\n",
    "- Partially Observable Environments: Agent가 간접적으로 Environment를 관찰한다. <br>\n",
    "예를 들면, 로봇이 카메라로 보는것은 그것의 절대 위치를 알려주지 않는 것과 포커 플레이 Agent는 오직 공개된 카드만 관찰할 수 있다는 것이다. <br>\n",
    "지금의 Agent state $\\neq$Environment state 이다.<br>\n",
    "일반적으로 이것을 Partially observable Markov decision process(POMDP)라고 한다.\n",
    "<br>\n",
    "Agent는 반드시 자신의 state representation $S_t^a$를 세워야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Inside an Reinforcement Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습은 다음의 요소를 가지고 있다.\n",
    "1. Policy\n",
    "2. Value functioin\n",
    "3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy는 Agent의 행동이다. 어떤 state에서 action으로 mapping해주는 역할이다. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3.2 Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미래보상을 예측해주는 함수다. 일반적으로 state가 좋은지 나쁜지 계산해준다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 Environment가 다음에 뭘할지 예측한다. 즉, 다음 state를 예측하거나 다음 reward를 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
